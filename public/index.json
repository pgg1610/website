[{"categories":null,"contents":"Synopsis: Catalysts comprising of metal nanoparticles dispersed on oxide supports have found applications in a vast number of chemical processes related to energy generation and environmental. In some cases, the interface between the metal and the supporting oxide is theorized to exhibit unique reactivity compared to just the metal or the oxide in isolation.\nWater-Gas Shift reaction (WGSR) is one such reaction which is expected to show sensitivity to such metal/oxide interfaces. We investigate WGSR with Pt nanoparticles supported MgO as a model system, to understand how the reaction proceed at these interfacial sites.\nKey takewaways:   The interfacial region greatly accelerate the water dissociation step which has an exceedingly high barrier on just the metal (Pt) or oxide (MgO) in isolation.\n  Partial poisoning of CO on the metal affects the over mechanism and it is important the model considers the effect explicitly\n  When developing reaction network schemes for multi-components systems the morphology of active site plays a crucial role with far-reaching consequences.\n  ","permalink":"/publications/ptmgo/","tags":["Kinetic modeling","Catalyst active-site design","DFT","Experimental Collaboration"],"title":"Catalysis at Metal/Oxide Interfaces: Water Gas Shift at Pt/MgO Boundaries"},{"categories":null,"contents":"Synopsis: Combination of experimental kinetics, state-of-the-art characterisation techiques and computation catalyst models to investigate the active site for propane dehydrogenation reaction. It is shown that alloying noble metals, platinum in this case, with another element (like vanadium) leads to changes in the catalytic performance. Through combined experiment/theory effort these performance and stability changes can be explained by perturbation in the electronic and structural properties of the alloys.\n","permalink":"/publications/pt3v/","tags":["Catalyst active-site design","DFT","Experimental Collaboration"],"title":"Electronic and Structural modification of Pt-V alloy and its consequences for Propane Dehydrogenation Catalysis"},{"categories":null,"contents":"Simple dropout to implement uncertainty estimates: Bayesian Neural Networks  Adapted from Deep Learning online course notes from NYU. Note link Paper about using Dropout as a Bayesian Approximation  Another notebook which uses PyTorch dropout: Link\n  Review article on application of uncertainty-quantification for small-molecule property prediction\n  New paper on evidential deep learning\n  In addition to predicting a value from a model it is also important to know the confidence in that prediction. Dropout is one way of estimating this. After multiple rounds of predictions, the mean and standard deviation in the prediction can be viewed as the prediction value and the corresponding confidence in the prediction. It is important to note that this is different from the error in the prediction. The model may have error in the prediction but could be precise in that value. It is similar to the idea of accuracy vs precision.\nType of uncertainties: Aleaotric and Epistemic uncertainty\n Aleatoric uncertainty captures noise inherent in the observations Epistemic uncertainty accounts for uncertainty in the model  The ideal way to measure epistemic uncertainty is to train many different models, each time using a different random seed and possibly varying hyperparameters. Then use all of them for each input and see how much the predictions vary. This is very expensive to do, since it involves repeating the whole training process many times. Fortunately, we can approximate the same effect in a less expensive way: by using dropout \u0026ndash; effectively training a huge ensemble of different models all at once. Each training sample is evaluated with a different dropout mask, corresponding to a different random subset of the connections in the full model. Usually we only perform dropout during training and use a single averaged mask for prediction. But instead, let\u0026rsquo;s use dropout for prediction too. We can compute the output for lots of different dropout masks, then see how much the predictions vary. This turns out to give a reasonable estimate of the epistemic uncertainty in the outputs\n# Training set m = 100 x = (torch.rand(m) - 0.5) * 20 #Returns a tensor filled with random numbers from a uniform distribution on the interval [0, 1) y = x * torch.sin(x) We define a simple feed-forward NN to learn the function.\n# Define a simple NN  class MLP(nn.Module): def __init__(self, hidden_layers=[20, 20], droprate=0.2, activation=\u0026#39;relu\u0026#39;): super(MLP, self).__init__() self.model = nn.Sequential() self.model.add_module(\u0026#39;input\u0026#39;, nn.Linear(1, hidden_layers[0])) if activation == \u0026#39;relu\u0026#39;: self.model.add_module(\u0026#39;relu0\u0026#39;, nn.ReLU()) elif activation == \u0026#39;tanh\u0026#39;: self.model.add_module(\u0026#39;tanh0\u0026#39;, nn.Tanh()) for i in range(len(hidden_layers)-1): self.model.add_module(\u0026#39;dropout\u0026#39;+str(i+1), nn.Dropout(p=droprate)) self.model.add_module(\u0026#39;hidden\u0026#39;+str(i+1), nn.Linear(hidden_layers[i], hidden_layers[i+1])) if activation == \u0026#39;relu\u0026#39;: self.model.add_module(\u0026#39;relu\u0026#39;+str(i+1), nn.ReLU()) elif activation == \u0026#39;tanh\u0026#39;: self.model.add_module(\u0026#39;tanh\u0026#39;+str(i+1), nn.Tanh()) self.model.add_module(\u0026#39;dropout\u0026#39;+str(i+2), nn.Dropout(p=droprate)) self.model.add_module(\u0026#39;final\u0026#39;, nn.Linear(hidden_layers[i+1], 1)) def forward(self, x): return self.model(x) # Define the model  net = MLP(hidden_layers=[200, 100, 80], droprate=0.1).to(device) #Move model to the GPU  print(net) # Objective and optimizer  criterion = nn.MSELoss() optimizer = optim.Adam(net.parameters(), lr=0.005, weight_decay=0.00001) # Training loop  for epoch in range(6000): x_dev = x.view(-1, 1).to(device) y_dev = y.view(-1, 1).to(device) y_hat = net(x_dev) loss = criterion(y_hat, y_dev) optimizer.zero_grad() loss.backward() optimizer.step() if epoch % 500 == 0: print(\u0026#39;Epoch[{}] - Loss:{}\u0026#39;.format(epoch, loss.item())) Once a NN with a dropout implemented is instantiated, the model is called multiple times to predict the output for a given input. While doing so it is important to ensure the model is in train() state.\n#Function to evaluate mean and std dev  def predict_reg(model, X, T=1000): model = model.train() Y_hat = list() with torch.no_grad(): for t in range(T): Y_hat.append(model(X.view(-1,1)).squeeze()) Y_hat = torch.stack(Y_hat) model = model.eval() with torch.no_grad(): Y_eval = model(X.view(-1,1)).squeeze() return Y_hat, Y_eval #Prediction on the points XX = torch.linspace(-11, 11, 1000) #New set of points  y_hat, y_eval = predict_reg(net, XX, T=1000) mean_y_hat = y_hat.mean(axis=0) std_y_hat = y_hat.std(axis=0) #Plotting  # Visualise mean and mean ± std -\u0026gt; confidence range fig, ax = plt.subplots(1,1, figsize=(10,10)) ax.plot(XX.numpy(), mean_y_hat.numpy(), \u0026#39;C1\u0026#39;, label=\u0026#39;prediction\u0026#39;) ax.fill_between(XX.numpy(), (mean_y_hat + std_y_hat).numpy(), (mean_y_hat - std_y_hat).numpy(), color=\u0026#39;C2\u0026#39;, label=\u0026#39;confidence\u0026#39;) ax.plot(x.numpy(), y.numpy(), \u0026#39;oC0\u0026#39;, label=\u0026#39;ground truth\u0026#39;) ax.plot(XX.numpy(), (XX * torch.sin(XX)).numpy(), \u0026#39;k\u0026#39;, label=\u0026#39;base function\u0026#39;) ax.axis(\u0026#39;equal\u0026#39;) plt.legend() ","permalink":"/blog/simple_dropout/","tags":["Python","PyTorch"],"title":"Simple Dropout using PyTorch"},{"categories":null,"contents":"List of links pertaining to eclectic areas of interest:\nSpecial Mentions  John Kitchin\u0026rsquo;s DFT book Andrew White\u0026rsquo;s Deep Learning for Molecules and Materials Notebook Fundamentals of Data Visualization  Linear Algebra + Stats  Khan Academy Lectures 3Blue1Brown\u0026rsquo;s Essence of Linear Algebra Series Intuitive guide to Linear Algebra StatQuest  Calculus  3Blue1Brown\u0026rsquo;s Calculus Series Khan Academy\u0026rsquo;s Multivariate Calculus  Data Science Blogs  Nate Silver\u0026rsquo;s 538 Pudding\u0026rsquo;s data viz Spurious Correlations Understanding uncertaintly Math3ma Blog  Learning Python  Automate Boring Stuff with Python Scientific programing with Python Visual Guide to Numpy Project Euler Github Repo for Projects Python DataScience Handbook Chris Albon\u0026rsquo;s notes  ML + Datascience:  Hands-on Machine Learning Jakevdp Python Datascience Notbook Cool visual intro to ML Blog entry for Deep learning crash course Distill Blog  Cool Courses:  Harvard\u0026rsquo;s CS 109 Stanford\u0026rsquo;s CS - CNN course MIT\u0026rsquo;s Intro to Deep Learning Google\u0026rsquo;s ML crash course Deep learning for molecules NYU\u0026rsquo;s PyTorch Deep learning CMU\u0026rsquo;s course on Computer Graphics MIT\u0026rsquo;s course on Computational Thinking  YouTubers List of YouTuber channels that never fail to inspire me\n1. Science and Technology\n Vsauce Sixty Symbols Tom Scott 3Blue1Brown Vertisasium Applied Science StatQuest MKBHD  2. Food\n J Kenji Lopez-Alt Binging with Babish Adam Ragusea Food Wishes  3.Videography and Design\n Casey Neistat Dan Mace Peter McKinnon Andrew Price - Blender CGMatter CG Figures - Scientific visualization in Blender  4. Journalism\n Dhruv Rahtee Johnny Harris Faye D\u0026rsquo;Souza  Misc:  Paul Graham Sam Altman Beauty of Science Maria Popova\u0026rsquo;s Brain Pickings Wait But Why Brad Feld\u0026rsquo;s Personal Blog Melthing Asphalt  Comics:  Xkcd Oatmeal  ","permalink":"/blog/helpful_links/","tags":["Guide"],"title":"Helpful links"},{"categories":null,"contents":"Indian food, like most of the food from the tropical region, uses a variety of different spices. Historically, the spices are thought to have been a way to increase the shelf-life and keep pests away. Whatever be the reason they sure are indespensible to food \u0026ndash; no matter which cuisine you\u0026rsquo;re making. Now, if you are planning to cook more Indian food lately, it would be great to know which spices would give you most bang for your buck. More so, which spices are often used together? What is the relation between each of them?\nRecently I found a dataset from Kaggle which tabulated 6000+ recipes from https://www.archanaskitchen.com/. Using this data as base collection of recipes representing most of the indian food, I analyze which spices occur most freqeuntly and which spices are most connected to each other.\n Dataset for Indian recipe: This dataset 6000+ recipe scrapped from: Dataset  The dataset from the Kaggle csv has entries as follows: Some entries in the TranslatedIngredients have non-english entries. To filter those out I made the following function:\ndef filter_english(string): try: string.encode(\u0026#39;utf-8\u0026#39;).decode(\u0026#39;ascii\u0026#39;) out = True except UnicodeDecodeError: out = False return out Next for consistent tabulation I needed a list of spices to look for. Wikipedia has a page on Indian spices which lists various spices used in Indian cuisin (Link). I use this list to search names of spices in the recipe entries.\nOne more step is editing the spices so that my string counter can find different versions of the same spice.\nspices_list = spices_list.str.replace(\u0026#39;amchoor\u0026#39;, \u0026#39;amchur/amchoor/mango extract\u0026#39;) \\ .replace(\u0026#39;asafoetida\u0026#39;, \u0026#39;asafetida/asafoetida/hing\u0026#39;) \\ .replace(\u0026#39;thymol/carom seed\u0026#39;, \u0026#39;ajwain/thymol/carom seed\u0026#39;) \\ .replace(\u0026#39;alkanet root\u0026#39;, \u0026#39;alkanet/alkanet root\u0026#39;) \\ .replace(\u0026#39;chilli powder\u0026#39;, \u0026#39;red chilli powder/chilli powder/kashmiri red chilli powder\u0026#39;) \\ .replace(\u0026#39;celery / radhuni seed\u0026#39;, \u0026#39;celery/radhuni seed\u0026#39;) \\ .replace(\u0026#39;bay leaf, indian bay leaf\u0026#39;, \u0026#39;bay leaf/bay leaves/tej patta\u0026#39;) \\ .replace(\u0026#39;curry tree or sweet neem leaf\u0026#39;, \u0026#39;curry leaf/curry leaves\u0026#39;) \\ .replace(\u0026#39;fenugreek leaf\u0026#39;, \u0026#39;fenugreek/kasoori methi\u0026#39;) \\ .replace(\u0026#39;nigella seed\u0026#39;, \u0026#39;nigella/black cumin\u0026#39;) \\ .replace(\u0026#39;ginger\u0026#39;, \u0026#39;dried ginger/ginger powder\u0026#39;) \\ .replace(\u0026#39;cloves\u0026#39;, \u0026#39;cloves/laung\u0026#39;) \\ .replace(\u0026#39;green cardamom\u0026#39;, \u0026#39;cardamom/green cardamom/black cardamom\u0026#39;)\\ .replace(\u0026#39;indian gooseberry\u0026#39;, \u0026#39;indian gooseberry/amla\u0026#39;)\\ .replace(\u0026#39;coriander seed\u0026#39;, \u0026#39;coriander seed/coriander powder\u0026#39;)\\ .replace(\u0026#39;cumin seed\u0026#39;, \u0026#39;cumin powder/cumin seeds/cumin/jeera\u0026#39;) For every food entries I consider the TranslatedIngredient column: I used regular expression to search for spice names in the entries\nimport re def search_spice(ingredient_string, spice_string): spice_list = spice_string.split(\u0026#39;/\u0026#39;) for _spice in spice_list: if re.search(_spice.lower(), ingredient_string.lower()): return True break food_spice_mix[spices_list.to_list()] = 42 for row, values in food_spice_mix.iterrows(): for spice_entry in spices_list: if search_spice(values[\u0026#39;TranslatedIngredients\u0026#39;], spice_entry): food_spice_mix.loc[row, spice_entry] = 1 else: food_spice_mix.loc[row, spice_entry] = 0 This way each food item is searched for a particular spice. A one-hot type binary list is made for every food item.\nBased on this binary entries we can create a adjacency matrix and a frequency plot counting the occurence of every spice in entirety of the food recipe corpus.\nspice_col_name = spices_list spice_adj_freq = pd.DataFrame(np.zeros(shape=(len(spices_list),len(spices_list))), columns= spice_col_name, index=spice_col_name) for row, value in food_spice_mix.iterrows(): for i in spice_col_name: for j in spice_col_name: if (value[i] == 1) \u0026amp; (value[j] == 1): spice_adj_freq.loc[i,j] += 1 spice_adj_freq = spice_adj_freq/len(food_spice_mix) * 100 Using frequency adjacency matrix we can plot a heatmap showing the pair-wise occurence for a given pair of spices. The idea with such an analysis is that if we can check the variation of Spice 1 with all the other spices in the list and compare that to Spice 2\u0026rsquo;s variation with all the other spices in the list, if spice 1 and spice 2 should have similar variation.\nThis map itself is quite interesting. The color intensity of each title shows the frequency that pair of spice occurred together in a recipe. Brighter the color higher their occurence together. Some prominent spice pairs which show similarity are:\n Curry leaves and Mustard seeds Tumeric and Chilli Powder  Some pair of spices never occur together:\n Saffron and Fenugreek seeds Nutmeg and Mustard Seeds  Those who cook or know indian recipes would see that these pairs make sense and thereby validate the correlation seen from corpus of Indian recipes.\nWith that analysis, we can go a step further and analyze this information in form of a circular network graph. Using this method of plotting, we can see the interactions between different spices.\nnodes_data = [(i, {\u0026#39;count\u0026#39;:spice_adj_freq.loc[i, i]}) for i in spice_col_name] edges_data = [] for i in temp_name: for j in temp_name: if i != j: if spice_adj_freq.loc[i,j] != 0.0: edges_data.append((i, j, {\u0026#39;weight\u0026#39;:spice_adj_freq.loc[i,j], \u0026#39;distance\u0026#39;:1})) #BUILD THE INITIAL FULL GRAPH G=nx.Graph() G.add_nodes_from(nodes_data) G.add_edges_from(edges_data) #Assigning weights to each node as per occurence  weights = [G[u][v][\u0026#39;weight\u0026#39;] for u,v in edges] w_arr = np.array(weights) norm_weight = (w_arr - w_arr.min())/(w_arr.max() - w_arr.min()) Finally a networkx circular graph is made where each node is a spice entry. Each edge between a pair of spice is a connection provided those two spices are found together in a recipe. The size of the node is the frequency of that spice to occur in all of 6000 food recipes. The thickness of the edge connecting a give spice-pair is the normalized frequency that pair occured among 6000 recipes. Representing the analysis this way we find few key takeaways:\n Tumeric, Mustard Seeds, Chilli Powder, Corriander Seeds, Cumin Seeds, Curry Leaves, Green Chillies, Asafoetida are the key spices in the Indian cuisine. Most recipes have strong tendancy to use Tumeric + Chilli Powder + Cumin Seed in them.  ","permalink":"/blog/indian_spices_network_analysis/","tags":["Web scraping","Python","Network analysis"],"title":"Analysis on Spice Use in Indian Food"},{"categories":null,"contents":"Analyzing averaging rating of Bollywood movies by using data from IMDb movie listing.\n","permalink":"/blog/imdb_scrapping/","tags":["Web scraping","Python"],"title":"Bollywood Movie Rating Analysis"},{"categories":null,"contents":"Generate list of companies in the S\u0026amp;P500 using BeautifulSoup. Next, use yfinance a alternative to Yahoo! Finance\u0026rsquo;s historical data API to extract stock information. Plot year-to-date return on certain stocks to check trends.\nList of SP500 companies is obtained from Wikipedia:\nwiki_url = \u0026#39;https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\u0026#39; response = get(wiki_url) html_soup = BeautifulSoup(response.text, \u0026#39;html.parser\u0026#39;) tab = html_soup.find(\u0026#34;table\u0026#34;,{\u0026#34;class\u0026#34;:\u0026#34;wikitable sortable\u0026#34;}) column_headings = [entry.text.strip() for entry in tab.findAll(\u0026#39;th\u0026#39;)] SP_500_dict = {keys:[] for keys in column_headings} Populate pandas dataframe with the listings:\nfor row_entry in tab.findAll(\u0026#39;tr\u0026#39;)[1:]: row_elements = row_entry.findAll(\u0026#39;td\u0026#39;) for key, _elements in zip(SP_500_dict.keys(), row_elements): SP_500_dict[key].append(_elements.text.strip()) SP_500_df = pd.DataFrame(SP_500_dict, columns=SP_500_dict.keys()) Plotting year-to-date (July 26th, 2020) estimate for the share prices.\nimport yfinance as yf START_DATE = \u0026#34;2020-01-01\u0026#34; END_DATE = \u0026#34;2020-07-26\u0026#34; yf_tickr = yf.Ticker(\u0026#39;ADBE\u0026#39;) _shares_outstanding = yf_tickr.info[\u0026#39;sharesOutstanding\u0026#39;] _previous_close = yf_tickr.info[\u0026#39;previousClose\u0026#39;] print(\u0026#39;Outstanding shares: {}\u0026#39;.format(_shares_outstanding)) print(\u0026#39;Market Cap: {} Million USD\u0026#39;.format((_shares_outstanding * _previous_close)/10**6)) df_tckr = yf_tickr.history(start=START_DATE, end=END_DATE, actions=False) df_tckr[\u0026#39;Market_Cap\u0026#39;] = df_tckr[\u0026#39;Open\u0026#39;] * _shares_outstanding df_tckr[\u0026#39;YTD\u0026#39;] = (df_tckr[\u0026#39;Open\u0026#39;] - df_tckr[\u0026#39;Open\u0026#39;][0]) * 100 / df_tckr[\u0026#39;Open\u0026#39;][0] Plotting this data for multiple companies.\ndef plot_market_cap(tickr_list, START_DATE, END_DATE): total_data = {} for tickr in tickr_list: total_data[tickr] = {} print(\u0026#39;Looking at: {}\u0026#39;.format(tickr)) yf_tickr = yf.Ticker(tickr) #try: # _shares_outstanding = yf_tickr.info[\u0026#39;sharesOutstanding\u0026#39;] #except(IndexError): # print(\u0026#39;Shares outstanding not found\u0026#39;) # _shares_outstanding = None df_tckr = yf_tickr.history(start=START_DATE, end=END_DATE, actions=False) df_tckr[\u0026#39;YTD\u0026#39;] = (df_tckr[\u0026#39;Open\u0026#39;] - df_tckr[\u0026#39;Open\u0026#39;][0]) * 100 / df_tckr[\u0026#39;Open\u0026#39;][0] total_data[tickr][\u0026#39;hist\u0026#39;] = df_tckr #total_data[tickr][\u0026#39;shares\u0026#39;] = _shares_outstanding time.sleep(np.random.randint(10)) return total_data ","permalink":"/blog/sp500/","tags":["Web scraping","Python"],"title":"Web scraping S\u0026P 500 data"},{"categories":null,"contents":" Notebook explaining the idea behind bayesian optimization alongside a small example showing its use. This notebook was adapted from Martin Krasser\u0026rsquo;s blogpost Good introductory write-up on Bayesian optimization here Nice lecture explaining the working of Gaussian Processes here  Setup If f (objective function) is cheap to evaluate we can sample various points and built a potential surface however, if the f is expensive \u0026ndash; like in case of first-principles electronic structure calculations, it is important to minimize the number of f calls and number of samples drawn from this evaluation. In that case, if an exact functional form for f is not available (that is, f behaves as a “black box”), what can we do?\nBayesian optimization proceeds by maintaining a probabilistic belief about f and designing a so called acquisition function to determine where to evaluate the function next. Bayesian optimization is particularly well-suited to global optimization problems where f is an expensive black-box function. The idea is the find \u0026ldquo;global\u0026rdquo; minimum with least number of steps. Incorporating prior beliefs about the underlying process and update the prior with samples draw from the model to better estimate the posterior. Model used for approximating the objective function is called the surrogate model.\nSurrogate model A popular surrogate model applied for Bayesian optimization, although strictly not required, are Gaussian Processes (GPs). These are used to define a prior beliefs about the objective function. The GP posterior is cheap to evaluate and is used to propose points in the search space where sampling is likely to yield an improvement. Herein, we could substitute this for a ANNs or other surrogate models.\nAcquisition functions Used to propose sampling points in the search space. Trade-off between exploitation vs exploration. Exploitation == sampling where objective function value is high; exploration == where uncertainty is high. Both correspond to high acquisition function value. The goal is the maximize the acquisition value to determine next sampling point.\nPopular acquisition functions:\n Maximum probability of improvement Expected improvement Upper confidence bound (UCB)  from scipy.stats import norm #Acquisition function modeled as Expected improvement  def expected_improvement(X, X_sample, Y_sample, gpr, xi=0.05): \u0026#39;\u0026#39;\u0026#39; Computes the EI at points X based on existing samples X_sample and Y_sample using a Gaussian process surrogate model. PARAMETERS: -------------------------------------- X: Points at which EI shall be computed (m x d). X_sample: Sample locations (n x d). Y_sample: Sample values (n x 1). gpr: A GaussianProcessRegressor fitted to samples. xi: Exploitation-exploration trade-off parameter. RETURNS: ------------------------------------- ei: Expected improvement at points X \u0026#39;\u0026#39;\u0026#39; mu, sigma = gpr.predict(X, return_std=True) mu_sample = gpr.predict(X_sample) sigma = sigma.reshape(-1, 1) mu_sample_opt = np.min(mu_sample) #Max for maximization  with np.errstate(divide=\u0026#39;warn\u0026#39;): imp = - (mu - mu_sample_opt - xi) #Positive for maximization  Z = imp / sigma ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z) ei[sigma == 0.0] = 0.0 return ei Bayesian optimization runs for few iterations. In each iteration, a row with two plots is produced.\n  The left plot shows: the noise-free objective function, the surrogate function which is the GP predicted mean value, the 95% confidence interval of the mean and the noisy samples obtained from the objective function so far.\n  The right plot shows the acquisition function value, EI in this case.\n  The vertical dashed line in both plots shows the proposed sampling point for the next iteration which corresponds to the maximum of the acquisition function.\nNote how the two initial samples at first drive search into the direction of the local maximum on the right side but exploration allows the algorithm to escape from that local optimum and find the minimum optimum on the left side. Also note how sampling point proposals often fall within regions of high uncertainty (exploration) and are not only driven by the highest surrogate function values (exploitation).\n","permalink":"/blog/bo/","tags":["Python","Optimization"],"title":"Bayesian Optimization using Gaussian Processes"},{"categories":null,"contents":" What? Sampling points from a distribution and plotting the frequency of the sample mean approaches a normal distribution.\n We start with some crazy distribution that has got nothing to do with a normal distribution. Sample points from that distribution with some arbitrary sample size, following which we plot the sample mean (or sample sum) on a frequency table \u0026ndash; repeat this lot of times (tending to infinity) we end up getting a normal distribution of sample means!\nFor all the unknown actions belonging to some probability distribution (SAME distribution is important) and sample points from them and average their values we end up getting a normal distribution. This is true for any distribution.\nMeanwhile, the Law of Large Numbers tells us that if we take a sample (n) observations of our random variable \u0026amp; avg the observation (mean)\u0026ndash; it will approach the expected value E(x) of the random variable.\nNice Khan Academy video explaining this. Link\nSmall python experiment to show this in action:\nDefining a discrete distribution Let\u0026rsquo;s assume we have a dice which is unfair and does not ever land on 3 and 5. Lands more on 2 and 6. We use Numpy\u0026rsquo;s random.choice module for this Link\ndice = np.arange(1,7) probabilities = [0.2, 0.3, 0.0, 0.2, 0.0, 0.3] # Draw sample size = n, take the mean and plot the frequencies  def sample_draw_mean(trials=1000, sample_size=1): sample_mean = [] for i in range(trials): sample = np.random.choice(dice, size=sample_size, p=probabilities, replace=True, ) sample_mean.append(np.mean(sample)) return sample_mean sns.distplot(sample_draw_mean(trials=1000, sample_size=1), bins=len(dice)); For sample size 1 it is seen that the frequency of rolling numbers of the die relate to the probability we have determined above. However we can start to define samples from that distribution wherein, instead of single number we draw ex. 4 numbers. We do this multiple times and plot the histogram of the mean.\nPlotting sampling distribution of sample mean As we keep plotting the frequency distribution for the sample mean it starts to approach the normal distribution! That\u0026rsquo;s the central limit theorem.\nAlso the mean of the distribution of the distribution is the population mean!\npopulation_mean = np.mean(dice) print(population_mean) = 3.5 ","permalink":"/blog/central_limit_theorem/","tags":["Python","Random"],"title":"Central limit theorem"},{"categories":null,"contents":"This project is adapted from Aurelien Geron\u0026rsquo;s Hands-on Machine Learning Book (github link).\nI build a regression model to predict median house values in Californian districts, given a number of features from these districts. More importantly, this project go through the basic building blocks when setting up any ML projects.\nFollowing are main steps to consider:  Formulating the problem, defining the boundary conditions Data acquisition Discover and visualize data / Data exploration to gain insight Prepare data for ML algorithm training and testing Explore various model architectures to use Select model and train it Fine-tuning the model  Problem statement: Prediction of district\u0026rsquo;s median housing price given all other metrics.\nA supervised learning task is where we are given \u0026lsquo;labelled\u0026rsquo; data for training purpose. Regression model to predict a continuous variable i.e. district median housing price. Given multiple features, this is a multi-class regression type problem. Univariate regression since a single output is estimated.\nDataset: Looking at quick statistics for the data at hand:\nDescribe is powerful subroutine since that allows us to check the stat summary of numerical attributes. The 25%-50%-75% entries for each column show corresponding percentiles. It indicates the value below which a given percentage of observations in a group of observations fall. For example, 25% of observation have median income below 2.56, 50% observations have median income below 3.53, and 75% observations have median income below 4.74. 25% \u0026ndash;\u0026gt; 1st Quartile, 50% \u0026ndash;\u0026gt; Median, 75% \u0026ndash;\u0026gt; 3rd Quartile\nPlot dataset housing.plot(kind=\u0026#34;scatter\u0026#34;, x=\u0026#34;longitude\u0026#34;, y=\u0026#34;latitude\u0026#34;, alpha=0.4, s=housing[\u0026#34;population\u0026#34;]/100, label=\u0026#34;population\u0026#34;, figsize=(15,10), c=\u0026#34;median_house_value\u0026#34;, cmap=plt.get_cmap(\u0026#34;jet\u0026#34;), colorbar=True, sharex=False) plt.legend() This is more interesting! We have now plotted the data with more information. Each data-point has two additional set of info apart of frequency of occurence. First being the color of the point is the median_house_value entry (option c). The radius of the data-point is the population of that district (option s). It can be seen that the housing prices are very much related to the location. The ones closer to the bay area are more expensive but need not be densely populated.\nLooking for simple correlations In addition to looking at the plot of housing price, we can check for simpler corealtions. Pearson\u0026rsquo;s correlation matrix is something which is in-built in pandas and can be directly used. It checks for linear correlation between every pair of feature provided in the data-set. It estimates the covariance of the two features and estimates whether the correlation is inverse, direct, or none.\ncorr_matrix=housing.corr() corr_matrix.style.background_gradient(cmap=\u0026#39;coolwarm\u0026#39;).set_precision(2) The correlation matrix suggests the amount of correlation between a pair of variables. When close to 1 it means a strong +ve correlation whereas, -1 means an inverse correlation. Looking at the correlation between median_house_values and other variables, we can see that there\u0026rsquo;s some correlation with median_income (0.68 \u0026ndash; so +ve), and with the latitude (-0.14 \u0026ndash; so an inverse relation).\n","permalink":"/blog/end2end_ml_project/","tags":["Python","Scikit-learn","Machine learning"],"title":"Property Price Prediction ML-model"},{"categories":null,"contents":"Convolutional neural network is used to train on the CIFAR-10 dataset using PyTorch.\nWhat does it consists of? The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\nThe dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\nWhat\u0026rsquo;s a Convolutional Neural Network (CNN)? Same as a neural network but are optimized for image analysis. Before training the weights and biases in the full-connected layer the training data is \u0026lsquo;screened and filtered\u0026rsquo; to tease out relevant features of each image by passing each image through a prescribed filter and \u0026lsquo;convolutions\u0026rsquo;. Think of it like passing a colored lens or fancy ink to selectively look at edges, contrasts, shapes in the image.\nFinally that a projection of that image is made by \u0026lsquo;pooling\u0026rsquo; which is a way of down-sampling the resulting convolution as a new data-point.\nGood reading links:\n http://cs231n.github.io/convolutional-networks/ https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/  ","permalink":"/blog/pytorch_cnn_example/","tags":["Python","PyTorch","Machine learning"],"title":"Implement CNN in PyTorch"},{"categories":null,"contents":"This tutorial is adapted from Jake VanderPlas\u0026rsquo;s example of SVM as given in his reference book: Python Data Science Handbook\nMotivation for Support Vector Machines We want to find a line/curve (in 2D) or a manifold (in n-D) that divides the class from each other. This is a type of Discriminative Classification\nConsider a simple case of classification task, in which the two classes of points are well separated. We can find region in space which best separates the data into two classes. The Support Vectors in Support Vector Machine are the (hyper)planes which lie at the edge of the individual classes. This idea is much easier to understand from 2D perspective.\n","permalink":"/blog/svm_usecase/","tags":["Python","Machine learning","Learning"],"title":"About Support Vector Machines"},{"categories":null,"contents":"High throughput screening of MaterialsProject dataset using pymatgen API for estimating perovskite bulk stability. For selected candidates, analyze their surface morphology using ab-initio thermodynamic analysis.\n","permalink":"/blog/perovskites/","tags":["Python","Thermodynamics","DFT"],"title":"Perovskite Stability Analysis"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml\n[outputs] home = [\u0026#34;HTML\u0026#34;, \u0026#34;JSON\u0026#34;] Searching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category\n... \u0026#34;contents\u0026#34;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026#34;tags\u0026#34;:{{ .Params.tags | jsonify }}{{end}}, \u0026#34;categories\u0026#34; : {{ .Params.categories | jsonify }}, ... Edit fuse.js options to Search static/js/search.js\nkeys: [ \u0026#34;title\u0026#34;, \u0026#34;contents\u0026#34;, \u0026#34;tags\u0026#34;, \u0026#34;categories\u0026#34; ] ","permalink":"/search/","tags":null,"title":"Search Results"}]