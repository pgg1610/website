[{"categories":null,"contents":"Synopsis: Catalysts comprising of metal nanoparticles dispersed on oxide supports have found applications in a vast number of chemical processes related to energy generation and environmental. In some cases, the interface between the metal and the supporting oxide is theorized to exhibit unique reactivity compared to just the metal or the oxide in isolation.\nWater-Gas Shift reaction (WGSR) is one such reaction which is expected to show sensitivity to such metal/oxide interfaces. We investigate WGSR with Pt nanoparticles supported MgO as a model system, to understand how the reaction proceed at these interfacial sites.\nKey takewaways:   The interfacial region greatly accelerate the water dissociation step which has an exceedingly high barrier on just the metal (Pt) or oxide (MgO) in isolation.\n  Partial poisoning of CO on the metal affects the over mechanism and it is important the model considers the effect explicitly\n  When developing reaction network schemes for multi-components systems the morphology of active site plays a crucial role with far-reaching consequences.\n  ","permalink":"/publications/ptmgo/","tags":["Kinetic modeling","Catalyst active-site design","DFT","Experimental Collaboration"],"title":"Catalysis at Metal/Oxide Interfaces: Water Gas Shift at Pt/MgO Boundaries"},{"categories":null,"contents":"Synopsis: Combination of experimental kinetics, state-of-the-art characterisation techiques and computation catalyst models to investigate the active site for propane dehydrogenation reaction. It is shown that alloying noble metals, platinum in this case, with another element (like vanadium) leads to changes in the catalytic performance. Through combined experiment/theory effort these performance and stability changes can be explained by perturbation in the electronic and structural properties of the alloys.\n","permalink":"/publications/pt3v/","tags":["Catalyst active-site design","DFT","Experimental Collaboration"],"title":"Electronic and Structural modification of Pt-V alloy and its consequences for Propane Dehydrogenation Catalysis"},{"categories":null,"contents":" Adapted from Deep Learning online course notes from NYU. Note link Paper about using Dropout as a Bayesian Approximation  In addition to predicting a value from a model it is also important to know the confidence in that prediction. Dropout is one way of estimating this. After multiple rounds of predictions, the mean and standard deviation in the prediction can be viewed as the prediction value and the corresponding confidence in the prediction. It is important to note that this is different from the error in the prediction. The model may have error in the prediction but could be precise in that value. It is similar to the idea of accuracy vs precision.\nWhen done with dropout \u0026ndash; the weights in the NN are scale by $\\frac{1}{1-r}$ to account for dropping of the weights\n# Define a simple NN  class MLP(nn.Module): def __init__(self, hidden_layers=[20, 20], droprate=0.2, activation=\u0026#39;relu\u0026#39;): super(MLP, self).__init__() self.model = nn.Sequential() self.model.add_module(\u0026#39;input\u0026#39;, nn.Linear(1, hidden_layers[0])) if activation == \u0026#39;relu\u0026#39;: self.model.add_module(\u0026#39;relu0\u0026#39;, nn.ReLU()) elif activation == \u0026#39;tanh\u0026#39;: self.model.add_module(\u0026#39;tanh0\u0026#39;, nn.Tanh()) for i in range(len(hidden_layers)-1): self.model.add_module(\u0026#39;dropout\u0026#39;+str(i+1), nn.Dropout(p=droprate)) self.model.add_module(\u0026#39;hidden\u0026#39;+str(i+1), nn.Linear(hidden_layers[i], hidden_layers[i+1])) if activation == \u0026#39;relu\u0026#39;: self.model.add_module(\u0026#39;relu\u0026#39;+str(i+1), nn.ReLU()) elif activation == \u0026#39;tanh\u0026#39;: self.model.add_module(\u0026#39;tanh\u0026#39;+str(i+1), nn.Tanh()) self.model.add_module(\u0026#39;dropout\u0026#39;+str(i+2), nn.Dropout(p=droprate)) self.model.add_module(\u0026#39;final\u0026#39;, nn.Linear(hidden_layers[i+1], 1)) def forward(self, x): return self.model(x) Once a NN with a dropout implemented is instantiated, the model is called multiple times to predict the output for a given input. While doing so it is important to ensure the model is in train() state.\ndef predict_reg(model, X, T=1000): model = model.train() Y_hat = list() with torch.no_grad(): for t in range(T): Y_hat.append(model(X.view(-1,1)).squeeze()) Y_hat = torch.stack(Y_hat) model = model.eval() with torch.no_grad(): Y_eval = model(X.view(-1,1)).squeeze() return Y_hat, Y_eval ","permalink":"/blog/simple_dropout/","tags":["Python","PyTorch"],"title":"Simple Dropout using PyTorch"},{"categories":null,"contents":"List of links pertaining to eclectic areas of interest:\nSpecial Mentions  John Kitchin\u0026rsquo;s DFT book Andrew Deep Learning for Molecules and Materials Fundamentals of Data Visualization  Linear Algebra + Stats  Khan Academy Lectures 3Blue1Brown\u0026rsquo;s Essence of Linear Algebra Series Intuitive guide to Linear Algebra StatQuest  Calculus  3Blue1Brown\u0026rsquo;s Calculus Series Khan Academy\u0026rsquo;s Multivariate Calculus  DataIsBeautiful  Nate Silver\u0026rsquo;s 538 Pudding\u0026rsquo;s data viz Spurious Correlations Understanding uncertaintly Math3ma Blog John Cook Allen Downey  Python  Automate Boring Stuff with Python Scientific programing with Python Visual Guide to Numpy Project Euler Github Repo for Projects Python DataScience Handbook  ML + datascience:  Hands-on Machine Learning Jakevdp Python Datascience Notbook Cool visual intro to ML Blog entry for Deep learning crash course Distill Blog  Learning Python  Python Practice Projects Project Euler Chris Albon\u0026rsquo;s notes  Cool Courses:  Harvard\u0026rsquo;s CS 109 Stanford\u0026rsquo;s CS - CNN course MIT\u0026rsquo;s Intro to Deep Learning Google\u0026rsquo;s ML crash course Deep learning for molecules NYU\u0026rsquo;s PyTorch Deep learning CMU\u0026rsquo;s course on Computer Graphics MIT\u0026rsquo;s course on Computational Thinking  YouTubers List of YouTuber channels that never fail to inspire me\n1. Science and Technology\n Vsauce Sixty Symbols Tom Scott 3Blue1Brown Vertisasium Applied Science StatQuest MKBHD  2. Food\n J Kenji Lopez-Alt Binging with Babish Adam Ragusea Food Wishes  3.Videography and Design\n Casey Neistat Dan Mace Peter McKinnon Andrew Price - Blender CGMatter CG Figures - Scientific visualization in Blender  4. Journalism\n Dhruv Rahtee Johnny Harris Faye D\u0026rsquo;Souza  Misc:  Paul Graham Sam Altman Beauty of Science Maria Popova\u0026rsquo;s Brain Pickings Wait But Why Brad Feld\u0026rsquo;s Personal Blog Melthing Asphalt  Comics:  Xkcd Oatmeal  ","permalink":"/blog/helpful_links/","tags":["Guide"],"title":"Helpful links"},{"categories":null,"contents":"Indian food, like most of the food from the tropical region, uses a variety of different spices. Historically, the spices are thought to have been a way to increase the shelf-life and keep pests away. Whatever be the reason they sure are indespensible to food \u0026ndash; no matter which cuisine you\u0026rsquo;re making. Now, if you are planning to cook more Indian food lately, it would be great to know which spices would give you most bang for your buck. More so, which spices are often used together? What is the relation between each of them?\nRecently I found a dataset from Kaggle which tabulated 6000+ recipes from https://www.archanaskitchen.com/. Using this data as base collection of recipes representing most of the indian food, I analyze which spices occur most freqeuntly and which spices are most connected to each other.\n Dataset for Indian recipe: This dataset 6000+ recipe scrapped from: Dataset  The dataset from the Kaggle csv has entries as follows: Some entries in the TranslatedIngredients have non-english entries. To filter those out I made the following function:\ndef filter_english(string): try: string.encode(\u0026#39;utf-8\u0026#39;).decode(\u0026#39;ascii\u0026#39;) out = True except UnicodeDecodeError: out = False return out Next for consistent tabulation I needed a list of spices to look for. Wikipedia has a page on Indian spices which lists various spices used in Indian cuisin (Link). I use this list to search names of spices in the recipe entries.\nOne more step is editing the spices so that my string counter can find different versions of the same spice.\nspices_list = spices_list.str.replace(\u0026#39;amchoor\u0026#39;, \u0026#39;amchur/amchoor/mango extract\u0026#39;) \\ .replace(\u0026#39;asafoetida\u0026#39;, \u0026#39;asafetida/asafoetida/hing\u0026#39;) \\ .replace(\u0026#39;thymol/carom seed\u0026#39;, \u0026#39;ajwain/thymol/carom seed\u0026#39;) \\ .replace(\u0026#39;alkanet root\u0026#39;, \u0026#39;alkanet/alkanet root\u0026#39;) \\ .replace(\u0026#39;chilli powder\u0026#39;, \u0026#39;red chilli powder/chilli powder/kashmiri red chilli powder\u0026#39;) \\ .replace(\u0026#39;celery / radhuni seed\u0026#39;, \u0026#39;celery/radhuni seed\u0026#39;) \\ .replace(\u0026#39;bay leaf, indian bay leaf\u0026#39;, \u0026#39;bay leaf/bay leaves/tej patta\u0026#39;) \\ .replace(\u0026#39;curry tree or sweet neem leaf\u0026#39;, \u0026#39;curry leaf/curry leaves\u0026#39;) \\ .replace(\u0026#39;fenugreek leaf\u0026#39;, \u0026#39;fenugreek/kasoori methi\u0026#39;) \\ .replace(\u0026#39;nigella seed\u0026#39;, \u0026#39;nigella/black cumin\u0026#39;) \\ .replace(\u0026#39;ginger\u0026#39;, \u0026#39;dried ginger/ginger powder\u0026#39;) \\ .replace(\u0026#39;cloves\u0026#39;, \u0026#39;cloves/laung\u0026#39;) \\ .replace(\u0026#39;green cardamom\u0026#39;, \u0026#39;cardamom/green cardamom/black cardamom\u0026#39;)\\ .replace(\u0026#39;indian gooseberry\u0026#39;, \u0026#39;indian gooseberry/amla\u0026#39;)\\ .replace(\u0026#39;coriander seed\u0026#39;, \u0026#39;coriander seed/coriander powder\u0026#39;)\\ .replace(\u0026#39;cumin seed\u0026#39;, \u0026#39;cumin powder/cumin seeds/cumin/jeera\u0026#39;) For every food entries I consider the TranslatedIngredient column: I used regular expression to search for spice names in the entries\nimport re def search_spice(ingredient_string, spice_string): spice_list = spice_string.split(\u0026#39;/\u0026#39;) for _spice in spice_list: if re.search(_spice.lower(), ingredient_string.lower()): return True break food_spice_mix[spices_list.to_list()] = 42 for row, values in food_spice_mix.iterrows(): for spice_entry in spices_list: if search_spice(values[\u0026#39;TranslatedIngredients\u0026#39;], spice_entry): food_spice_mix.loc[row, spice_entry] = 1 else: food_spice_mix.loc[row, spice_entry] = 0 This way each food item is searched for a particular spice. A one-hot type binary list is made for every food item.\nBased on this binary entries we can create a adjacency matrix and a frequency plot counting the occurence of every spice in entirety of the food recipe corpus.\nspice_col_name = spices_list spice_adj_freq = pd.DataFrame(np.zeros(shape=(len(spices_list),len(spices_list))), columns= spice_col_name, index=spice_col_name) for row, value in food_spice_mix.iterrows(): for i in spice_col_name: for j in spice_col_name: if (value[i] == 1) \u0026amp; (value[j] == 1): spice_adj_freq.loc[i,j] += 1 spice_adj_freq = spice_adj_freq/len(food_spice_mix) * 100 Using frequency adjacency matrix we can plot a heatmap showing the pair-wise occurence for a given pair of spices. The idea with such an analysis is that if we can check the variation of Spice 1 with all the other spices in the list and compare that to Spice 2\u0026rsquo;s variation with all the other spices in the list, if spice 1 and spice 2 should have similar variation.\nThis map itself is quite interesting. The color intensity of each title shows the frequency that pair of spice occurred together in a recipe. Brighter the color higher their occurence together. Some prominent spice pairs which show similarity are:\n Curry leaves and Mustard seeds Tumeric and Chilli Powder  Some pair of spices never occur together:\n Saffron and Fenugreek seeds Nutmeg and Mustard Seeds  Those who cook or know indian recipes would see that these pairs make sense and thereby validate the correlation seen from corpus of Indian recipes.\nWith that analysis, we can go a step further and analyze this information in form of a circular network graph.\nnodes_data = [(i, {\u0026#39;count\u0026#39;:spice_adj_freq.loc[i, i]}) for i in spice_col_name] edges_data = [] for i in temp_name: for j in temp_name: if i != j: if spice_adj_freq.loc[i,j] != 0.0: edges_data.append((i, j, {\u0026#39;weight\u0026#39;:spice_adj_freq.loc[i,j], \u0026#39;distance\u0026#39;:1})) #BUILD THE INITIAL FULL GRAPH G=nx.Graph() G.add_nodes_from(nodes_data) G.add_edges_from(edges_data) #Assigning weights to each node as per occurence  weights = [G[u][v][\u0026#39;weight\u0026#39;] for u,v in edges] w_arr = np.array(weights) norm_weight = (w_arr - w_arr.min())/(w_arr.max() - w_arr.min()) Finally a networkx circular graph is made where each node is a spice entry. Each edge between a pair of spice is a connection provided those two spices are found together in a recipe. The size of the node is the frequency of that spice to occur in all of 6000 food recipes. The thickness of the edge connecting a give spice-pair is the normalized frequency that pair occured among 6000 recipes. Representing the analysis this way we find few key takeaways:\n Tumeric, Mustard Seeds, Chilli Powder, Corriander Seeds, Cumin Seeds, Curry Leaves, Green Chillies, Asafoetida are the key spices in the Indian cuisine. Most recipes have strong tendancy to use Tumeric + Chilli Powder + Cumin Seed in them.  ","permalink":"/blog/indian_spices_network_analysis/","tags":["Web scraping","Python","Network analysis"],"title":"Analysis on Spice Use in Indian Food"},{"categories":null,"contents":"Analyzing averaging rating of Bollywood movies by using data from IMDb movie listing.\n","permalink":"/blog/imdb_scrapping/","tags":["Web scraping","Python"],"title":"Bollywood Movie Rating Analysis"},{"categories":null,"contents":"Generate list of companies in the S\u0026amp;P500 using BeautifulSoup. Next, use yfinance (Yahoo Finance) module to extract stock information. Plot year-to-date return on certain stocks to check trends.\n","permalink":"/blog/sp500/","tags":["Web scraping","Python"],"title":"Web scraping S\u0026P 500 data"},{"categories":null,"contents":" Notebook explaining the idea behind bayesian optimization alongside a small example showing its use. This notebook was adapted from Martin Krasser\u0026rsquo;s blogpost Good introductory write-up on Bayesian optimization here Nice lecture explaining the working of Gaussian Processes here  ","permalink":"/blog/bo/","tags":["Python","Optimization"],"title":"Bayesian Optimization using Gaussian Processes"},{"categories":null,"contents":" What? Sampling points from a distribution and plotting the frequency of the sample mean approaches a normal distribution.\n We start with some crazy distribution that has got nothing to do with a normal distribution. Sample points from that distribution with some arbitrary sample size, following which we plot the sample mean (or sample sum) on a frequency table \u0026ndash; repeat this lot of times (tending to infinity) we end up getting a normal distribution of sample means!\nFor all the unknown actions belonging to some probability distribution (SAME distribution is important) and sample points from them and average their values we end up getting a normal distribution. This is true for any distribution.\nMeanwhile, the Law of Large Numbers tells us that if we take a sample (n) observations of our random variable \u0026amp; avg the observation (mean)\u0026ndash; it will approach the expected value E(x) of the random variable.\nNice Khan Academy video explaining this. Link\nSmall python experiment to show this in action:\nDefining a discrete distribution Let\u0026rsquo;s assume we have a dice which is unfair and does not ever land on 3 and 5. Lands more on 2 and 6. We use Numpy\u0026rsquo;s random.choice module for this Link\ndice = np.arange(1,7) probabilities = [0.2, 0.3, 0.0, 0.2, 0.0, 0.3] # Draw sample size = n, take the mean and plot the frequencies  def sample_draw_mean(trials=1000, sample_size=1): sample_mean = [] for i in range(trials): sample = np.random.choice(dice, size=sample_size, p=probabilities, replace=True, ) sample_mean.append(np.mean(sample)) return sample_mean sns.distplot(sample_draw_mean(trials=1000, sample_size=1), bins=len(dice)); For sample size 1 it is seen that the frequency of rolling numbers of the die relate to the probability we have determined above. However we can start to define samples from that distribution wherein, instead of single number we draw ex. 4 numbers. We do this multiple times and plot the histogram of the mean.\nPlotting sampling distribution of sample mean As we keep plotting the frequency distribution for the sample mean it starts to approach the normal distribution! That\u0026rsquo;s the central limit theorem.\nAlso the mean of the distribution of the distribution is the population mean!\npopulation_mean = np.mean(dice) print(population_mean) = 3.5 ","permalink":"/blog/central_limit_theorem/","tags":["Python","Random"],"title":"Central limit theorem"},{"categories":null,"contents":"This project is adapted from Aurelien Geron\u0026rsquo;s Hands-on Machine Learning Book (github link). We build a regression model to predict median house values in Californian districts, given a number of features from these districts. More importantly, this project go through the basic building blocks when setting up any ML projects.\nFollowing are main steps to consider:\n Formulating the problem, defining the boundary conditions Data acquisition Discover and visualize data / Data exploration to gain insight Prepare data for ML algorithm training and testing Explore various model architectures to use Select model and train it Fine-tuning the model  ","permalink":"/blog/end2end_ml_project/","tags":["Python","Scikit-learn","Machine learning"],"title":"Property Price Prediction ML-model"},{"categories":null,"contents":"Convolutional neural network is used to train on the CIFAR-10 dataset using PyTorch.\nWhat does it consists of? The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\nThe dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\nWhat\u0026rsquo;s a Convolutional Neural Network (CNN)? Same as a neural network but are optimized for image analysis. Before training the weights and biases in the full-connected layer the training data is \u0026lsquo;screened and filtered\u0026rsquo; to tease out relevant features of each image by passing each image through a prescribed filter and \u0026lsquo;convolutions\u0026rsquo;. Think of it like passing a colored lens or fancy ink to selectively look at edges, contrasts, shapes in the image.\nFinally that a projection of that image is made by \u0026lsquo;pooling\u0026rsquo; which is a way of down-sampling the resulting convolution as a new data-point.\nGood reading links:\n http://cs231n.github.io/convolutional-networks/ https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/  ","permalink":"/blog/pytorch_cnn_example/","tags":["Python","PyTorch","Machine learning"],"title":"Implement CNN in PyTorch"},{"categories":null,"contents":"This tutorial is adapted from Jake VanderPlas\u0026rsquo;s example of SVM as given in his reference book: Python Data Science Handbook\nMotivation for Support Vector Machines We want to find a line/curve (in 2D) or a manifold (in n-D) that divides the class from each other. This is a type of Discriminative Classification\nConsider a simple case of classification task, in which the two classes of points are well separated. We can find region in space which best separates the data into two classes. The Support Vectors in Support Vector Machine are the (hyper)planes which lie at the edge of the individual classes. This idea is much easier to understand from 2D perspective.\n","permalink":"/blog/svm_usecase/","tags":["Python","Machine learning","Learning"],"title":"About Support Vector Machines"},{"categories":null,"contents":"High throughput screening of MaterialsProject dataset using pymatgen API for estimating perovskite bulk stability. For selected candidates, analyze their surface morphology using ab-initio thermodynamic analysis.\n","permalink":"/blog/perovskites/","tags":["Python","Thermodynamics","DFT"],"title":"Perovskite Stability Analysis"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml\n[outputs] home = [\u0026#34;HTML\u0026#34;, \u0026#34;JSON\u0026#34;] Searching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category\n... \u0026#34;contents\u0026#34;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026#34;tags\u0026#34;:{{ .Params.tags | jsonify }}{{end}}, \u0026#34;categories\u0026#34; : {{ .Params.categories | jsonify }}, ... Edit fuse.js options to Search static/js/search.js\nkeys: [ \u0026#34;title\u0026#34;, \u0026#34;contents\u0026#34;, \u0026#34;tags\u0026#34;, \u0026#34;categories\u0026#34; ] ","permalink":"/search/","tags":null,"title":"Search Results"}]