[{"categories":null,"contents":"Synopsis: In the past several years, the U.S. Chemical Safety Board has found an increase in the frequency of laboratory accidents and injuries. A framework to document, assess, and mitigate hazards is a critical starting point for ensuring safe laboratory practices. To address this requirement, I was part of a team working on developing Reactive Hazards Evaluation Analysis and Compilation Tool (RHEACT), an online platform to compile and scrutinize hazards-related information, was developed. This work was jointly supported by the CISTAR consortium and the Purdue Process Safety and Assurance Center (P2SAC)\nWhen planning an experiment, the researchers provide RHEACT: (1) information about the chemicals involved in the reaction, in the form of Safety Data Sheets (SDS), and (2) operating parameters of the reaction. Through the user-supplied SDS, an operational hazard matrix and a chemical compatibility matrix are generated. In addition, adiabatic temperature rise of the reaction is estimated to ensure that the chemistry is within user-controlled bounds. The user is provided with a broad initial evaluation of potential hazards and is notified of safety concerns associated with the reaction before conducting the experiment.\nDuring the development of the tool, I was involved in following tasks:\n Develop the PDF parser that will read, extract, and store relevant chemical information from material safety datasheets (MSDS) Build front-end GUI using React Build web automation routine for generating chemical compatibility matrix using Selenium.  ","permalink":"/publications/rheact/","tags":["Laboratory Safety","Online Tool","Web scraping","React"],"title":"Promoting a Safe Laboratory Environment Using the Reactive Hazard Evaluation and Analysis Compilation Tools"},{"categories":null,"contents":"Synopsis: Catalysts comprising of metal nanoparticles dispersed on oxide supports have found applications in a vast number of chemical processes related to energy generation and environmental. In some cases, the interface between the metal and the supporting oxide is theorized to exhibit unique reactivity compared to just the metal or the oxide in isolation.\nWater-Gas Shift reaction (WGSR) is one such reaction which is expected to show sensitivity to such metal/oxide interfaces. We investigate WGSR with Pt nanoparticles supported MgO as a model system, to understand how the reaction proceed at these interfacial sites.\nKey takewaways:   The interfacial region greatly accelerate the water dissociation step which has an exceedingly high barrier on just the metal (Pt) or oxide (MgO) in isolation.\n  Partial poisoning of CO on the metal affects the over mechanism and it is important the model considers the effect explicitly\n  When developing reaction network schemes for multi-components systems the morphology of active site plays a crucial role with far-reaching consequences.\n  ","permalink":"/publications/ptmgo/","tags":["Kinetic modeling","Catalyst active-site design","DFT","Experimental Collaboration"],"title":"Catalysis at Metal/Oxide Interfaces: Water Gas Shift at Pt/MgO Boundaries"},{"categories":null,"contents":"Synopsis: Combination of experimental kinetics, state-of-the-art characterisation techiques and computation catalyst models to investigate the active site for propane dehydrogenation reaction. It is shown that alloying noble metals, platinum in this case, with another element (like vanadium) leads to changes in the catalytic performance. Through combined experiment/theory effort these performance and stability changes can be explained by perturbation in the electronic and structural properties of the alloys.\n","permalink":"/publications/pt3v/","tags":["Catalyst active-site design","DFT","Experimental Collaboration"],"title":"Electronic and Structural modification of Pt-V alloy and its consequences for Propane Dehydrogenation Catalysis"},{"categories":null,"contents":"Simple example to illustrate the utility and working of graph neural networks.\nA natural way to represent information in a structured form is as a graph. A graph is a data structure describing a collection of entities, represented as nodes, and their pairwise relationships, represented as edges. Think of it as a mathematical abstraction to present relational data.\nGraphs are everywhere: social networks, the world wide web, street maps, knowledge bases used in search engines, and even chemical molecules are frequently represented as a set of entities and relations between them.\nMachine learning deals with the question of how we can build systems and design algorithms that learn from data and experience (e.g., by interacting with an environment), which is in contrast to the traditional approach in science where systems are explicitly programmed to follow a precisely outlined sequence of instructions. The problem of learning is commonly approached by fitting a model to data with the goal that this learned model will generalize to new data or experiences.\nFurthermore, Graph network learning provides a promising combination of two ideas:\n(1) having strong relational inductive bias for a data structure which is amenable for graph representation\n(2) find hidden features/representation that can be \u0026lsquo;learned\u0026rsquo; with more data.\nThis idea is explored in further details in this fairly exhaustive review of graph networks\n In this post we will look at a simple case of using Graph Neural Networks to aid labeling and separating nodes in the graph structured data.\nTask: Generating embedding for a graph dataset using a Graph Convolution Neural Network (GCN) on Zachary\u0026rsquo;s Karate Club Network.\n Dataset: Zachary W. (1977). An information flow model for conflict and fission in small groups. Journal of Anthropological Research, 33, 452-473  The dataset describes the social interaction of 34 members and the communities that rise from it, 4 in this case. Each members of the club is defined as a node. Each node is connected to other members in the club. This connection would determine the final grouping of the community in 4 separate labels.\nHow is GNN useful here? Consider a situation where: We knew how every one is connected to each other in the club. However we only know the final label of only 4 members in the club. That means, out of 34 members we know only 4 members\u0026rsquo; final label. Can we use the node connections and the GCN idea to predict and cluster other members of the group? In addition currently the data is structured in a tuple of (node, edge connections) can we use graph neural network to estimate lower dimensional embedding to describe each node?\nSetup Karate Club Dataset:\n======================\nNumber of graphs: 1\nNumber of features: 34\nNumber of classes: 4\nfrom torch_geometric.utils import to_networkx from torch_geometric.datasets import KarateClub dataset = KarateClub() G = to_networkx(dataset[0], to_undirected=True) fig, ax = plt.subplots(1,1,figsize=(10,10)) #Plot the dataset using Networkx nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), node_size=10**3, with_labels=True, node_color=data.y, cmap=\u0026#34;Set2\u0026#34;, ax=ax) Like introduced in the previous section: Each node in the graph is a person. Every person has an associated number (index) and the community or club they would eventually join. There are 4 clubs in total. Each node has an associated edges with other nodes in the network based on connections. Now having that connection we can construct an adjacency matrix. The environment of each node can be used to predict the final community the user would end up in.\nWe can re-express this problem as given the nodes and the connections which club would each node join. We can see if the GCN network can predict the targets properly, and if the targets can be used to find low dimensional representation for the graph.\nTo describe each members in the network a one-hot encoding is used where the entry corresponding to the index of the node is 1 and everything else is 0. Sorting these nodes based on index we get a identity matrix (34, 34). More elaborate schemes can be thought of to describe the node entries. Like in case of molecule property prediction each atom which would be a node can be expressed as combination of chemical properties.\nNode features used to describe the 34 members:\ntensor([[1., 0., 0., ..., 0., 0., 0.], [0., 1., 0., ..., 0., 0., 0.], [0., 0., 1., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 1., 0., 0.], [0., 0., 0., ..., 0., 1., 0.], [0., 0., 0., ..., 0., 0., 1.]]) Next, every node in the graph is attached to other nodes. This information is stored in adjacency matrix. Self-connections are by default labelled 0. Every row of the adjacency matrix shows node connections\nNode adjacency matrix used to describe the connections of 34 nodes:\ntensor([[0., 1., 1., ..., 1., 0., 0.], [1., 0., 1., ..., 0., 0., 0.], [1., 1., 0., ..., 0., 1., 0.], ..., [1., 0., 0., ..., 0., 1., 1.], [0., 0., 1., ..., 1., 0., 1.], [0., 0., 0., ..., 1., 1., 0.]]) For example: as shown figure at the top if we look at node (16) it is connected to node (5,6). Hence in the adjacency matrix the entries belonging to node (16) are index (5,6) are 1, other that all other entries are 0\nnp.where( Karate_adjacency[16] == 1 )[0] \u0026gt;\u0026gt; array([5, 6])  Besides storing whole adjacency matrix information where many entries would be 0 and not important, sometimes edge connection information is stored in a coordinate format. In this format the edge-connections are described in tuples and only non-zero entries are populated. This way the representation is sparse and not memory intensive.\n  Graph neural network implementation Given the graph, node features, and the node connections with other nodes we can construct the graph convolution operation to use the geometric information and predict properties of the graph and the nodes.\n1. Tipf\u0026rsquo;s Graph Convolution Implementation\nBasic implementation of GCN used when making the neural network.\nclass GCNConv(nn.Module): def __init__(self, A, input_dims, output_dims): super(GCNConv, self).__init__() \u0026#39;\u0026#39;\u0026#39; As per Tipf explanation: https://tkipf.github.io/graph-convolutional-networks/ https://arxiv.org/abs/1609.02907 PARAMETERS: --------------- A: numpy.array, Adjacency matrix for the graph object input_dims: int, Input dimensions for the NN params output_dims: int, Output dimensions for the NN params RETURNS: --------------- out: torch.Tensor, N x output for the NN prediction \u0026#39;\u0026#39;\u0026#39; torch.manual_seed(42) self.A_hat = A + torch.eye(A.size(0)) self.D = torch.diag(torch.sum(A,1)) #Diagonal node-degree matrix  self.D = self.D.inverse().sqrt() self.A_hat = torch.mm( torch.mm(self.D, self.A_hat), self.D ) self.W = nn.Parameter(torch.rand(input_dims, output_dims, requires_grad=True)) def forward(self, X): out = torch.tanh(torch.mm( torch.mm(self.A_hat, X), self.W )) return out Building the total neural network model:\nThe model consists of 3 GCN parts which you can think of going upto 3 nearest neighbors to account for the local information. Finally the updated nodes features post each convolution is fed in to a fully-connected neural network where the output is one of the 4 classes.\nclass Net(torch.nn.Module): def __init__(self, A, nfeat, nhid, c): super(Net, self).__init__() self.conv1 = GCNConv(A, nfeat, nhid) self.conv2 = GCNConv(A, nhid, nhid) self.conv3 = GCNConv(A, nhid, 2) self.linear = nn.Linear(2, nhid) def forward(self,X): H0 = self.conv1(X) H1 = self.conv2(H0) H2 = self.conv3(H1) out = self.linear(H2) return H2, out Training the model for i in range(1000): e, out = simple_GCN(node_features) optimizer.zero_grad() #reset optimizer cache  loss=criterion(out[data.train_mask], data.y[data.train_mask]) #estimate loss on ONLY 4 nodes -- mask to identify the nodes  loss.backward() #initiate back-prop  optimizer.step() #update the NN weights  if i % 100==0: print(\u0026#34;Step: {} Cross Entropy Loss = {}\u0026#34;.format(i, loss.item())) output_, _ = simple_GCN(node_features) visualize_graph(data, output_) First forward-pass result:\nAt first visualizing the output there is not clear distinction in the nodes. The coloring is done as the ground truth labels.\nVisualizing post-GNN training:\nOnce the weight in the GCN defined above are trained on the node connections and node label and ONLY 4 nodes, the clustering of all nodes in 4 groups becomes apparent. The Class 2 which is the light blue group is the most distinct and it is also the most well separated of the group in the original representation too. There is some overlap in the Class 1 3 4 which is captured in the low dimensional as well. However given information of final label of only 4 nodes the GCN does a nice job of clustering all the nodes in their respective 4 clusters.\n 2. PyTorch Geometric Implementation\nThis part is adapted from PyTorch Geometric\u0026rsquo;s tutorial page. Link\nIn this case we use the GCN module built in the PyTorch Geometric package.\nfrom torch_geometric.nn import GCNConv from torch_geometric.utils import add_self_loops, degree class GCN(torch.nn.Module): def __init__(self, graph_data): super(GCN, self).__init__() torch.manual_seed(42) self.graph_data = graph_data self.conv1 = GCNConv(self.graph_data.num_features, 4) self.conv2 = GCNConv(4, 4) self.conv3 = GCNConv(4, 2) self.classifier = nn.Linear(2, self.graph_data.num_classes) def forward(self, node_features, edge_index): h = self.conv1(node_features, edge_index) h = h.tanh() h = self.conv2(h, edge_index) h = h.tanh() h = self.conv3(h, edge_index) h = h.tanh() # Final GNN embedding space. # Apply a final (linear) classifier. out = self.classifier(h) return out, h Training the model criterion = torch.nn.CrossEntropyLoss() # Define loss criterion. optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # Define optimizer. def train(data): optimizer.zero_grad() # Clear gradients. out, h = model(node_features, data.edge_index) # Perform a single forward pass. loss = criterion(out[data.train_mask], data.y[data.train_mask]) # Compute the loss solely based on the training nodes. loss.backward() # Derive gradients. optimizer.step() # Update parameters based on gradients. return loss, h for epoch in range(1000): loss, h = train(data) if epoch % 100 == 0: print(\u0026#34;Step: {} Cross Entropy Loss = {}\u0026#34;.format(epoch, loss.item())) visualize_graph(data, h) First forward-pass result:\nVisualizing post-GNN training:\n","permalink":"/blog/graph_nn/","tags":["Python","Pytorch","Graphs"],"title":"Simple Graph Neural Network"},{"categories":null,"contents":"Simple dropout to implement uncertainty estimates: Bayesian Neural Networks  Adapted from Deep Learning online course notes from NYU. Note link Paper about using Dropout as a Bayesian Approximation  Another notebook which uses PyTorch dropout: Link\n  Review article on application of uncertainty-quantification for small-molecule property prediction\n  New paper on evidential deep learning\n  In addition to predicting a value from a model it is also important to know the confidence in that prediction. Dropout is one way of estimating this. After multiple rounds of predictions, the mean and standard deviation in the prediction can be viewed as the prediction value and the corresponding confidence in the prediction. It is important to note that this is different from the error in the prediction. The model may have error in the prediction but could be precise in that value. It is similar to the idea of accuracy vs precision.\nType of uncertainties: Aleaotric and Epistemic uncertainty\n Aleatoric uncertainty captures noise inherent in the observations Epistemic uncertainty accounts for uncertainty in the model  The ideal way to measure epistemic uncertainty is to train many different models, each time using a different random seed and possibly varying hyperparameters. Then use all of them for each input and see how much the predictions vary. This is very expensive to do, since it involves repeating the whole training process many times. Fortunately, we can approximate the same effect in a less expensive way: by using dropout \u0026ndash; effectively training a huge ensemble of different models all at once. Each training sample is evaluated with a different dropout mask, corresponding to a different random subset of the connections in the full model. Usually we only perform dropout during training and use a single averaged mask for prediction. But instead, let\u0026rsquo;s use dropout for prediction too. We can compute the output for lots of different dropout masks, then see how much the predictions vary. This turns out to give a reasonable estimate of the epistemic uncertainty in the outputs\n# Training set m = 100 x = (torch.rand(m) - 0.5) * 20 #Returns a tensor filled with random numbers from a uniform distribution on the interval [0, 1) y = x * torch.sin(x) We define a simple feed-forward NN to learn the function.\n# Define a simple NN  class MLP(nn.Module): def __init__(self, hidden_layers=[20, 20], droprate=0.2, activation=\u0026#39;relu\u0026#39;): super(MLP, self).__init__() self.model = nn.Sequential() self.model.add_module(\u0026#39;input\u0026#39;, nn.Linear(1, hidden_layers[0])) if activation == \u0026#39;relu\u0026#39;: self.model.add_module(\u0026#39;relu0\u0026#39;, nn.ReLU()) elif activation == \u0026#39;tanh\u0026#39;: self.model.add_module(\u0026#39;tanh0\u0026#39;, nn.Tanh()) for i in range(len(hidden_layers)-1): self.model.add_module(\u0026#39;dropout\u0026#39;+str(i+1), nn.Dropout(p=droprate)) self.model.add_module(\u0026#39;hidden\u0026#39;+str(i+1), nn.Linear(hidden_layers[i], hidden_layers[i+1])) if activation == \u0026#39;relu\u0026#39;: self.model.add_module(\u0026#39;relu\u0026#39;+str(i+1), nn.ReLU()) elif activation == \u0026#39;tanh\u0026#39;: self.model.add_module(\u0026#39;tanh\u0026#39;+str(i+1), nn.Tanh()) self.model.add_module(\u0026#39;dropout\u0026#39;+str(i+2), nn.Dropout(p=droprate)) self.model.add_module(\u0026#39;final\u0026#39;, nn.Linear(hidden_layers[i+1], 1)) def forward(self, x): return self.model(x) # Define the model  net = MLP(hidden_layers=[200, 100, 80], droprate=0.1).to(device) #Move model to the GPU  print(net) # Objective and optimizer  criterion = nn.MSELoss() optimizer = optim.Adam(net.parameters(), lr=0.005, weight_decay=0.00001) # Training loop  for epoch in range(6000): x_dev = x.view(-1, 1).to(device) y_dev = y.view(-1, 1).to(device) y_hat = net(x_dev) loss = criterion(y_hat, y_dev) optimizer.zero_grad() loss.backward() optimizer.step() if epoch % 500 == 0: print(\u0026#39;Epoch[{}] - Loss:{}\u0026#39;.format(epoch, loss.item())) Once a NN with a dropout implemented is instantiated, the model is called multiple times to predict the output for a given input. While doing so it is important to ensure the model is in train() state.\n#Function to evaluate mean and std dev  def predict_reg(model, X, T=1000): model = model.train() Y_hat = list() with torch.no_grad(): for t in range(T): Y_hat.append(model(X.view(-1,1)).squeeze()) Y_hat = torch.stack(Y_hat) model = model.eval() with torch.no_grad(): Y_eval = model(X.view(-1,1)).squeeze() return Y_hat, Y_eval #Prediction on the points XX = torch.linspace(-11, 11, 1000) #New set of points  y_hat, y_eval = predict_reg(net, XX, T=1000) mean_y_hat = y_hat.mean(axis=0) std_y_hat = y_hat.std(axis=0) #Plotting  # Visualise mean and mean ± std -\u0026gt; confidence range fig, ax = plt.subplots(1,1, figsize=(10,10)) ax.plot(XX.numpy(), mean_y_hat.numpy(), \u0026#39;C1\u0026#39;, label=\u0026#39;prediction\u0026#39;) ax.fill_between(XX.numpy(), (mean_y_hat + std_y_hat).numpy(), (mean_y_hat - std_y_hat).numpy(), color=\u0026#39;C2\u0026#39;, label=\u0026#39;confidence\u0026#39;) ax.plot(x.numpy(), y.numpy(), \u0026#39;oC0\u0026#39;, label=\u0026#39;ground truth\u0026#39;) ax.plot(XX.numpy(), (XX * torch.sin(XX)).numpy(), \u0026#39;k\u0026#39;, label=\u0026#39;base function\u0026#39;) ax.axis(\u0026#39;equal\u0026#39;) plt.legend() ","permalink":"/blog/simple_dropout/","tags":["Python","PyTorch"],"title":"Simple Dropout using PyTorch"},{"categories":null,"contents":"List of links pertaining to eclectic areas of interest:\nSpecial Mentions  John Kitchin\u0026rsquo;s DFT book Andrew White\u0026rsquo;s Deep Learning for Molecules and Materials Notebook Fundamentals of Data Visualization  Linear Algebra + Stats  Khan Academy Lectures 3Blue1Brown\u0026rsquo;s Essence of Linear Algebra Series Intuitive guide to Linear Algebra StatQuest  Calculus  3Blue1Brown\u0026rsquo;s Calculus Series Khan Academy\u0026rsquo;s Multivariate Calculus  Data Science Blogs  Nate Silver\u0026rsquo;s 538 Pudding\u0026rsquo;s data viz Spurious Correlations Understanding uncertaintly Math3ma Blog  Learning Python  Automate Boring Stuff with Python Scientific programing with Python Visual Guide to Numpy Project Euler Github Repo for Projects Python DataScience Handbook Chris Albon\u0026rsquo;s notes  ML + Datascience:  Hands-on Machine Learning Jakevdp Python Datascience Notbook Cool visual intro to ML Blog entry for Deep learning crash course Distill Blog  Cool Courses:  Harvard\u0026rsquo;s CS 109 Stanford\u0026rsquo;s CS - CNN course MIT\u0026rsquo;s Intro to Deep Learning Google\u0026rsquo;s ML crash course Deep learning for molecules NYU\u0026rsquo;s PyTorch Deep learning CMU\u0026rsquo;s course on Computer Graphics MIT\u0026rsquo;s course on Computational Thinking  YouTubers List of YouTuber channels that never fail to inspire me\n1. Science and Technology\n Vsauce Sixty Symbols Tom Scott 3Blue1Brown Vertisasium Applied Science StatQuest MKBHD  2. Food\n J Kenji Lopez-Alt Binging with Babish Adam Ragusea Food Wishes  3.Videography and Design\n Casey Neistat Dan Mace Peter McKinnon Andrew Price - Blender CGMatter CG Figures - Scientific visualization in Blender  4. Journalism\n Dhruv Rahtee Johnny Harris Faye D\u0026rsquo;Souza  Misc:  Paul Graham Sam Altman Beauty of Science Maria Popova\u0026rsquo;s Brain Pickings Wait But Why Brad Feld\u0026rsquo;s Personal Blog Melthing Asphalt Robert Heaton Astral Codex Ten  Comics:  Xkcd Oatmeal Catana Comics  ","permalink":"/blog/helpful_links/","tags":["Guide"],"title":"Helpful links"},{"categories":null,"contents":"Indian food, like most of the food from the tropical region, uses a variety of different spices. Historically, the spices are thought to have been a way to increase the shelf-life and keep pests away. Whatever be the reason they sure are indespensible to food \u0026ndash; no matter which cuisine you\u0026rsquo;re making. Now, if you are planning to cook more Indian food lately, it would be great to know which spices would give you most bang for your buck. More so, which spices are often used together? What is the relation between each of them?\nRecently I found a dataset from Kaggle which tabulated 6000+ recipes from https://www.archanaskitchen.com/. Using this data as base collection of recipes representing most of the indian food, I analyze which spices occur most freqeuntly and which spices are most connected to each other.\n Dataset for Indian recipe: This dataset 6000+ recipe scrapped from: Dataset  The dataset from the Kaggle csv has entries as follows: Some entries in the TranslatedIngredients have non-english entries. To filter those out I made the following function:\ndef filter_english(string): try: string.encode(\u0026#39;utf-8\u0026#39;).decode(\u0026#39;ascii\u0026#39;) out = True except UnicodeDecodeError: out = False return out Next for consistent tabulation I needed a list of spices to look for. Wikipedia has a page on Indian spices which lists various spices used in Indian cuisin (Link). I use this list to search names of spices in the recipe entries.\nOne more step is editing the spices so that my string counter can find different versions of the same spice.\nspices_list = spices_list.str.replace(\u0026#39;amchoor\u0026#39;, \u0026#39;amchur/amchoor/mango extract\u0026#39;) \\ .replace(\u0026#39;asafoetida\u0026#39;, \u0026#39;asafetida/asafoetida/hing\u0026#39;) \\ .replace(\u0026#39;thymol/carom seed\u0026#39;, \u0026#39;ajwain/thymol/carom seed\u0026#39;) \\ .replace(\u0026#39;alkanet root\u0026#39;, \u0026#39;alkanet/alkanet root\u0026#39;) \\ .replace(\u0026#39;chilli powder\u0026#39;, \u0026#39;red chilli powder/chilli powder/kashmiri red chilli powder\u0026#39;) \\ .replace(\u0026#39;celery / radhuni seed\u0026#39;, \u0026#39;celery/radhuni seed\u0026#39;) \\ .replace(\u0026#39;bay leaf, indian bay leaf\u0026#39;, \u0026#39;bay leaf/bay leaves/tej patta\u0026#39;) \\ .replace(\u0026#39;curry tree or sweet neem leaf\u0026#39;, \u0026#39;curry leaf/curry leaves\u0026#39;) \\ .replace(\u0026#39;fenugreek leaf\u0026#39;, \u0026#39;fenugreek/kasoori methi\u0026#39;) \\ .replace(\u0026#39;nigella seed\u0026#39;, \u0026#39;nigella/black cumin\u0026#39;) \\ .replace(\u0026#39;ginger\u0026#39;, \u0026#39;dried ginger/ginger powder\u0026#39;) \\ .replace(\u0026#39;cloves\u0026#39;, \u0026#39;cloves/laung\u0026#39;) \\ .replace(\u0026#39;green cardamom\u0026#39;, \u0026#39;cardamom/green cardamom/black cardamom\u0026#39;)\\ .replace(\u0026#39;indian gooseberry\u0026#39;, \u0026#39;indian gooseberry/amla\u0026#39;)\\ .replace(\u0026#39;coriander seed\u0026#39;, \u0026#39;coriander seed/coriander powder\u0026#39;)\\ .replace(\u0026#39;cumin seed\u0026#39;, \u0026#39;cumin powder/cumin seeds/cumin/jeera\u0026#39;) For every food entries I consider the TranslatedIngredient column: I used regular expression to search for spice names in the entries\nimport re def search_spice(ingredient_string, spice_string): spice_list = spice_string.split(\u0026#39;/\u0026#39;) for _spice in spice_list: if re.search(_spice.lower(), ingredient_string.lower()): return True break food_spice_mix[spices_list.to_list()] = 42 for row, values in food_spice_mix.iterrows(): for spice_entry in spices_list: if search_spice(values[\u0026#39;TranslatedIngredients\u0026#39;], spice_entry): food_spice_mix.loc[row, spice_entry] = 1 else: food_spice_mix.loc[row, spice_entry] = 0 This way each food item is searched for a particular spice. A one-hot type binary list is made for every food item.\nBased on this binary entries we can create a adjacency matrix and a frequency plot counting the occurence of every spice in entirety of the food recipe corpus.\nspice_col_name = spices_list spice_adj_freq = pd.DataFrame(np.zeros(shape=(len(spices_list),len(spices_list))), columns= spice_col_name, index=spice_col_name) for row, value in food_spice_mix.iterrows(): for i in spice_col_name: for j in spice_col_name: if (value[i] == 1) \u0026amp; (value[j] == 1): spice_adj_freq.loc[i,j] += 1 spice_adj_freq = spice_adj_freq/len(food_spice_mix) * 100 Using frequency adjacency matrix we can plot a heatmap showing the pair-wise occurence for a given pair of spices. The idea with such an analysis is that if we can check the variation of Spice 1 with all the other spices in the list and compare that to Spice 2\u0026rsquo;s variation with all the other spices in the list, if spice 1 and spice 2 should have similar variation.\nThis map itself is quite interesting. The color intensity of each title shows the frequency that pair of spice occurred together in a recipe. Brighter the color higher their occurence together. Some prominent spice pairs which show similarity are:\n Curry leaves and Mustard seeds Tumeric and Chilli Powder  Some pair of spices never occur together:\n Saffron and Fenugreek seeds Nutmeg and Mustard Seeds  Those who cook or know indian recipes would see that these pairs make sense and thereby validate the correlation seen from corpus of Indian recipes.\nWith that analysis, we can go a step further and analyze this information in form of a circular network graph. Using this method of plotting, we can see the interactions between different spices.\nnodes_data = [(i, {\u0026#39;count\u0026#39;:spice_adj_freq.loc[i, i]}) for i in spice_col_name] edges_data = [] for i in temp_name: for j in temp_name: if i != j: if spice_adj_freq.loc[i,j] != 0.0: edges_data.append((i, j, {\u0026#39;weight\u0026#39;:spice_adj_freq.loc[i,j], \u0026#39;distance\u0026#39;:1})) #BUILD THE INITIAL FULL GRAPH G=nx.Graph() G.add_nodes_from(nodes_data) G.add_edges_from(edges_data) #Assigning weights to each node as per occurence  weights = [G[u][v][\u0026#39;weight\u0026#39;] for u,v in edges] w_arr = np.array(weights) norm_weight = (w_arr - w_arr.min())/(w_arr.max() - w_arr.min()) Finally a networkx circular graph is made where each node is a spice entry. Each edge between a pair of spice is a connection provided those two spices are found together in a recipe. The size of the node is the frequency of that spice to occur in all of 6000 food recipes. The thickness of the edge connecting a give spice-pair is the normalized frequency that pair occured among 6000 recipes. Representing the analysis this way we find few key takeaways:\n Tumeric, Mustard Seeds, Chilli Powder, Corriander Seeds, Cumin Seeds, Curry Leaves, Green Chillies, Asafoetida are the key spices in the Indian cuisine. Most recipes have strong tendancy to use Tumeric + Chilli Powder + Cumin Seed in them.  ","permalink":"/blog/indian_spices_network_analysis/","tags":["Web scraping","Python","Network analysis"],"title":"Analysis on Spice Use in Indian Food"},{"categories":null,"contents":"Analyzing averaging rating of Bollywood movies by using data from IMDb movie listing.\n","permalink":"/blog/imdb_scrapping/","tags":["Web scraping","Python"],"title":"Bollywood Movie Rating Analysis"},{"categories":null,"contents":"Generate list of companies in the S\u0026amp;P500 using BeautifulSoup. Next, use yfinance a alternative to Yahoo! Finance\u0026rsquo;s historical data API to extract stock information. Plot year-to-date return on certain stocks to check trends.\nList of SP500 companies is obtained from Wikipedia:\nwiki_url = \u0026#39;https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\u0026#39; response = get(wiki_url) html_soup = BeautifulSoup(response.text, \u0026#39;html.parser\u0026#39;) tab = html_soup.find(\u0026#34;table\u0026#34;,{\u0026#34;class\u0026#34;:\u0026#34;wikitable sortable\u0026#34;}) column_headings = [entry.text.strip() for entry in tab.findAll(\u0026#39;th\u0026#39;)] SP_500_dict = {keys:[] for keys in column_headings} Populate pandas dataframe with the listings:\nfor row_entry in tab.findAll(\u0026#39;tr\u0026#39;)[1:]: row_elements = row_entry.findAll(\u0026#39;td\u0026#39;) for key, _elements in zip(SP_500_dict.keys(), row_elements): SP_500_dict[key].append(_elements.text.strip()) SP_500_df = pd.DataFrame(SP_500_dict, columns=SP_500_dict.keys()) Plotting year-to-date (July 26th, 2020) estimate for the share prices.\nimport yfinance as yf START_DATE = \u0026#34;2020-01-01\u0026#34; END_DATE = \u0026#34;2020-07-26\u0026#34; yf_tickr = yf.Ticker(\u0026#39;ADBE\u0026#39;) _shares_outstanding = yf_tickr.info[\u0026#39;sharesOutstanding\u0026#39;] _previous_close = yf_tickr.info[\u0026#39;previousClose\u0026#39;] print(\u0026#39;Outstanding shares: {}\u0026#39;.format(_shares_outstanding)) print(\u0026#39;Market Cap: {} Million USD\u0026#39;.format((_shares_outstanding * _previous_close)/10**6)) df_tckr = yf_tickr.history(start=START_DATE, end=END_DATE, actions=False) df_tckr[\u0026#39;Market_Cap\u0026#39;] = df_tckr[\u0026#39;Open\u0026#39;] * _shares_outstanding df_tckr[\u0026#39;YTD\u0026#39;] = (df_tckr[\u0026#39;Open\u0026#39;] - df_tckr[\u0026#39;Open\u0026#39;][0]) * 100 / df_tckr[\u0026#39;Open\u0026#39;][0] Plotting this data for multiple companies.\ndef plot_market_cap(tickr_list, START_DATE, END_DATE): total_data = {} for tickr in tickr_list: total_data[tickr] = {} print(\u0026#39;Looking at: {}\u0026#39;.format(tickr)) yf_tickr = yf.Ticker(tickr) #try: # _shares_outstanding = yf_tickr.info[\u0026#39;sharesOutstanding\u0026#39;] #except(IndexError): # print(\u0026#39;Shares outstanding not found\u0026#39;) # _shares_outstanding = None df_tckr = yf_tickr.history(start=START_DATE, end=END_DATE, actions=False) df_tckr[\u0026#39;YTD\u0026#39;] = (df_tckr[\u0026#39;Open\u0026#39;] - df_tckr[\u0026#39;Open\u0026#39;][0]) * 100 / df_tckr[\u0026#39;Open\u0026#39;][0] total_data[tickr][\u0026#39;hist\u0026#39;] = df_tckr #total_data[tickr][\u0026#39;shares\u0026#39;] = _shares_outstanding time.sleep(np.random.randint(10)) return total_data ","permalink":"/blog/sp500/","tags":["Web scraping","Python"],"title":"Web scraping S\u0026P 500 data"},{"categories":null,"contents":" Notebook explaining the idea behind bayesian optimization alongside a small example showing its use. This notebook was adapted from Martin Krasser\u0026rsquo;s blogpost Good introductory write-up on Bayesian optimization here Nice lecture explaining the working of Gaussian Processes here  Setup If f (objective function) is cheap to evaluate we can sample various points and built a potential surface however, if the f is expensive \u0026ndash; like in case of first-principles electronic structure calculations, it is important to minimize the number of f calls and number of samples drawn from this evaluation. In that case, if an exact functional form for f is not available (that is, f behaves as a “black box”), what can we do?\nBayesian optimization proceeds by maintaining a probabilistic belief about f and designing a so called acquisition function to determine where to evaluate the function next. Bayesian optimization is particularly well-suited to global optimization problems where f is an expensive black-box function. The idea is the find \u0026ldquo;global\u0026rdquo; minimum with least number of steps. Incorporating prior beliefs about the underlying process and update the prior with samples draw from the model to better estimate the posterior. Model used for approximating the objective function is called the surrogate model.\nSurrogate model A popular surrogate model applied for Bayesian optimization, although strictly not required, are Gaussian Processes (GPs). These are used to define a prior beliefs about the objective function. The GP posterior is cheap to evaluate and is used to propose points in the search space where sampling is likely to yield an improvement. Herein, we could substitute this for a ANNs or other surrogate models.\nAcquisition functions Used to propose sampling points in the search space. Trade-off between exploitation vs exploration. Exploitation == sampling where objective function value is high; exploration == where uncertainty is high. Both correspond to high acquisition function value. The goal is the maximize the acquisition value to determine next sampling point.\nPopular acquisition functions:\n Maximum probability of improvement Expected improvement Lower/Upper confidence bound (UCB)  1. Expected Improvement\ndef EI(X_new, gpr, delta, noisy, minimize_objective): \u0026#34;\u0026#34;\u0026#34; Compute the expected improvement at points X_new, from a Gaussian process surrogate model fit to observed data (X_sample, Y_sample). Arguments --------- X_new : array_like; shape (num_new_pts, input_dimension) Locations at which to compute expected improvement. gpr : GaussianProcessRegressor Regressor object, pre-fit to the sample data via the command gpr.fit(X_sample, Y_sample). delta : float Trade-off parameter for exploration vs. exploitation. Must be a non-negative value. A value of zero corresponds to pure ex- ploitation, with more exploration at larger values of delta. noisy : bool If True, assumes a noisy model and predicts the expected outputs at X_sample, rather than using Y_sample. minimize_objective : bool Designates whether the objective function is to be minimized or maximized. By default, minimization is assumed. In either case, the expected improvement is defined such that its value should be maximized. Returns ------- ei : np.ndarray; shape (num_points,) The expected improvement at each of the points in X_new. \u0026#34;\u0026#34;\u0026#34; if delta \u0026lt; 0.0: raise ValueError(\u0026#34;Exploration parameter must be non-negative.\u0026#34;) if minimize_objective: best = np.min sign = -1.0 else: best = np.max sign = 1.0 (mu, sigma) = gpr.predict(X_new, return_std = True) if (mu.ndim \u0026gt; 1 and mu.shape[1] \u0026gt; 1) or mu.ndim \u0026gt; 2: raise RuntimeError(\u0026#34;Invalid shape for predicted \u0026#34; \u0026#34;mean: %s\u0026#34; % (mu.shape,)) else: mu = mu.flatten() sigma = np.maximum(1e-15, sigma.flatten()) # Bump small variances to prevent divide-by-zero. if noisy: mu_sample = gpr.predict(gpr.X_train_) best_y = best(mu_sample) else: best_y = best(gpr.y_train_) improvement = sign*(mu - best_y + delta) Z = improvement/sigma return improvement*stats.norm.cdf(Z) + sigma*stats.norm.pdf(Z) 2. Lower Confidence Bound\ndef LCB(X_new, gpr, sigma): \u0026#34;\u0026#34;\u0026#34; Compute the lower confidence bound at points X_new, from a Gaussian process surrogate model fit to observed data (X_sample, Y_sample). Arguments --------- X_new : array_like; shape (num_new_pts, input_dimension) Locations at which to compute confidence bound. gpr : GaussianProcessRegressor Regressor object, pre-fit to the sample data via the command gpr.fit(X_sample, Y_sample). sigma : float Trade-off parameter for exploration vs. exploitation. Must be a non-negative value. A value of zero corresponds to pure exploitation, with more exploration at larger values of sigma. Returns ------- lcb : np.ndarray; shape (num_points,) The lower confidence bound at each of the points in X_new. \u0026#34;\u0026#34;\u0026#34; if sigma \u0026lt; 0.0: raise ValueError(\u0026#34;Exploration parameter must be non-negative.\u0026#34;) (mean, std_dev) = gpr.predict(X_new, return_std = True) if (mean.ndim \u0026gt; 1 and mean.shape[1] \u0026gt; 1) or mean.ndim \u0026gt; 2: raise RuntimeError(\u0026#34;Invalid shape for predicted \u0026#34; \u0026#34;mean: %s\u0026#34; % (mean.shape,)) else: mean = mean.flatten() return mean - sigma*std_dev Objective function Objective function f we are interested in optimizing is the Egg Carton function which has quite peculiar shape, as seen in the schematic below. While there are local \u0026lsquo;swiggles\u0026rsquo; the overall function tends to a lower value around x = (4,6). We want to see if bayesian optimization can find this minimum value by optimizing not the ground function but rather a surrogate function which hypothetically would be \u0026lsquo;cheaper\u0026rsquo; to evaluate and optimize on.\nThe plot shown below has two main things: 1. The ground truth function which is the Egg carton function (shown by the black line) 2. The randomly sampled points which have some error built into them. Think of this like a sampling of surface with some error built-into the measuring the device, so it wont accurate sample the ground-truth function. We will use this \u0026lsquo;noisy\u0026rsquo; function for optimization.\nPlot for the objective function:\ndef egg_carton(x, f_noise = 0.0): x = np.asarray(x) return np.sin(4.25*x) + 0.25*(x - 4.8)**2.0 + f_noise * np.random.randn(*x.shape) Initial points are sampled from numpy\u0026rsquo;s random number in a uniform distribution:\nnum_sample_points = 10 noise_ = 0.1 generator = np.random.default_rng(42) x_sample = generator.uniform(low, high, size = (num_sample_points, 1)) y_sample = objective(x_sample, noise_) Bayesian optimization  Fit a surrogate function on initial points  Bayesian optimization runs for few iterations.\nFor the inital points and the function value a GPR model as implemented in the sklearn.gaussian_process.GaussianProcessRegressor module is used. The prediction from the GPR is then used to optimize the acquisition function \u0026ndash; Expected Improvement Criterion or Lower Confidence Bound.\nRunning a few more iterations: In total the noisy estimation of the ground-truth is conducted on 30 additional points. It is evident from the plot that most of those points are near the x = (4,6) since that is the minimum value region for the function.\n","permalink":"/blog/bo/","tags":["Python","Optimization","Bayesian Stats"],"title":"Bayesian Optimization using Gaussian Processes"},{"categories":null,"contents":" What? Sampling points from a distribution and plotting the frequency of the sample mean approaches a normal distribution.\n We start with some crazy distribution that has got nothing to do with a normal distribution. Sample points from that distribution with some arbitrary sample size, following which we plot the sample mean (or sample sum) on a frequency table \u0026ndash; repeat this lot of times (tending to infinity) we end up getting a normal distribution of sample means!\nFor all the unknown actions belonging to some probability distribution (SAME distribution is important) and sample points from them and average their values we end up getting a normal distribution. This is true for any distribution.\nMeanwhile, the Law of Large Numbers tells us that if we take a sample (n) observations of our random variable \u0026amp; avg the observation (mean)\u0026ndash; it will approach the expected value E(x) of the random variable.\nNice Khan Academy video explaining this. Link\nSmall python experiment to show this in action:\nDefining a discrete distribution Let\u0026rsquo;s assume we have a dice which is unfair and does not ever land on 3 and 5. Lands more on 2 and 6. We use Numpy\u0026rsquo;s random.choice module for this Link\ndice = np.arange(1,7) probabilities = [0.2, 0.3, 0.0, 0.2, 0.0, 0.3] # Draw sample size = n, take the mean and plot the frequencies  def sample_draw_mean(trials=1000, sample_size=1): sample_mean = [] for i in range(trials): sample = np.random.choice(dice, size=sample_size, p=probabilities, replace=True, ) sample_mean.append(np.mean(sample)) return sample_mean sns.distplot(sample_draw_mean(trials=1000, sample_size=1), bins=len(dice)); For sample size 1 it is seen that the frequency of rolling numbers of the die relate to the probability we have determined above. However we can start to define samples from that distribution wherein, instead of single number we draw ex. 4 numbers. We do this multiple times and plot the histogram of the mean.\nPlotting sampling distribution of sample mean As we keep plotting the frequency distribution for the sample mean it starts to approach the normal distribution! That\u0026rsquo;s the central limit theorem.\nAlso the mean of the distribution of the distribution is the population mean!\npopulation_mean = np.mean(dice) print(population_mean) = 3.5 ","permalink":"/blog/central_limit_theorem/","tags":["Python","Random"],"title":"Central limit theorem"},{"categories":null,"contents":"End-to-end Machine Learning Project This project is adapted from Aurelien Geron\u0026rsquo;s Hands-on Machine Learning Book\nI build a regression model to predict median house values in Californian districts, given a number of features from these districts. More importantly, this project go through the basic building blocks when setting up any ML projects.\nFollowing are main steps to consider:  Formulating the problem, defining the boundary conditions Data acquisition Discover and visualize data / Data exploration to gain insight Prepare data for ML algorithm training and testing Explore various model architectures to use Select model and train it Fine-tuning the model  Step 1: Formulate the problem Prediction of district\u0026rsquo;s median housing price given all other metrics. A supervised learning task is where we are given \u0026lsquo;labelled\u0026rsquo; data for training purpose. Regression model to predict a continuous variable i.e. district median housing price. Given multiple features, this is a multi-class regression type problem. Univariate regression since a single output is estimated.\nStep 2: Get the data import pandas as pd import numpy as np %config InlineBackend.figure_format = \u0026#39;retina\u0026#39; import matplotlib as mpl import matplotlib.pyplot as plt #Load the dataset  housing = pd.read_csv(\u0026#39;housing.csv\u0026#39;) housing.sample(7) Each row presents one district. Each of these districts has 10 attributes (features).\nhousing.info() \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 20640 entries, 0 to 20639 Data columns (total 10 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 longitude 20640 non-null float64 1 latitude 20640 non-null float64 2 housing_median_age 20640 non-null float64 3 total_rooms 20640 non-null float64 4 total_bedrooms 20433 non-null float64 5 population 20640 non-null float64 6 households 20640 non-null float64 7 median_income 20640 non-null float64 8 median_house_value 20640 non-null float64 9 ocean_proximity 20640 non-null object dtypes: float64(9), object(1) memory usage: 1.6+ MB  One thing to notice in this dataset is the number of total_bedroom entries is different from other entries. This suggests there are some missing entries or null in the dataset.\n#To show the null elements (if any in the total_bedroom entries) housing[housing.total_bedrooms.isnull()] For categorical entries (here, ocean_proximity entries) we can find out the entries and their number using the value_counts(). We can do this for any entry we wish but makes more sense for categorical entries.\nhousing[\u0026#34;ocean_proximity\u0026#34;].value_counts() \u0026lt;1H OCEAN 9136 INLAND 6551 NEAR OCEAN 2658 NEAR BAY 2290 ISLAND 5 Name: ocean_proximity, dtype: int64  housing.describe().round(2) Describe is powerful subroutine since that allows us to check the stat summary of numerical attributes\nThe 25%-50%-75% entries for each column show corresponding percentiles. It indicates the value below which a given percentage of observations in a group of observations fall. For example, 25% of observation have median income below 2.56, 50% observations have median income below 3.53, and 75% observations have median income below 4.74. 25% \u0026ndash;\u0026gt; 1st Quartile, 50% \u0026ndash;\u0026gt; Median, 75% \u0026ndash;\u0026gt; 3rd Quartile\nhousing.hist(bins=50,figsize=(20,20)) Few observations from the Histogram plots, again remember each row is an entry for an ENTIRE district:\n NOTICE: From the dataset\u0026rsquo;s source disclaimer: The housing_median_value, housing_median_age, median_income_value are capped at an arbitrary value.\n   From latitute and longitude plots there seems to be lots of district in four particular locations (34,37 \u0026ndash; latitude) and (-120,-118 \u0026ndash; longitude). We cannot comment on the exact location but only one on these pairs giving most data.\n  We see a tighter distribution for total_rooms, total_bedrooms, and population but spread for house_value and an intresting spike at its end.\n  Small spike at the end of median_income plot suggests presence of small group of affluent families but interestingly that spike does not correlate with the spike in the house_value (More high-end property entries than more \u0026ldquo;rich\u0026rdquo; people in a district)\n  Finally, the dataset is tail-heavy that is they extend further to the right from the median which might make modeling using some ML algorithm a bit chanellenging.\nFew entries should be scaled such that the distribution is more normal.\nCreate a test-set This ensures that this is the data on which training, testing occurs and we do not try overfitting to account for all the variance in the data. Typical 20% of data-points are randomly chosen.\ndef split_train_test(data,test_ratio): shuffled_indices=np.random.permutation(len(data)) test_set_size=int(len(data)*test_ratio) test_indices=shuffled_indices[:test_set_size] train_indices=shuffled_indices[test_set_size:] return(data.iloc[train_indices],data.iloc[test_indices]) #To ensure we get similar results at each run -- if not initiated every successive will give more random  #shuffled indices risking the possibility of the algo seeing the entire dataset!  np.random.seed(42) #Random seed set to 42 for no particular reason  #but just cause its the answer to the Ultimate Question of Life, The Universe, and Everything train_set, test_set = split_train_test(housing, 0.2) print(len(train_set), \u0026#34;train +\u0026#34;, len(test_set), \u0026#34;test\u0026#34;) 16512 train + 4128 test  Better way is to have an instance identifier (like id) for each entry to distingusih each entry and see if its sampled or not.\nfrom zlib import crc32 def test_set_check(identifier, test_ratio): return crc32(np.int64(identifier)) \u0026amp; 0xffffffff \u0026lt; test_ratio * 2**32 def split_train_test_by_id(data, test_ratio, id_column): ids = data[id_column] in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio)) return data.loc[~in_test_set], data.loc[in_test_set] The dataset currently doesnt have inherent id. We could use the row index as id. Or we could use an ad-hoc unique identifier as an interim id.\n#HOUSING DATA WITH ID as ROW INDEX housing_with_id = housing.reset_index() # adds an `index` column train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \u0026#34;index\u0026#34;) #HOUSING DATA WITH ID AS COMBO OF LAT AND LONG.  housing_with_id[\u0026#34;id\u0026#34;] = housing[\u0026#34;longitude\u0026#34;] * 1000 + housing[\u0026#34;latitude\u0026#34;] train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \u0026#34;id\u0026#34;) #SCIKIT-LEARN IMPLEMENTATION #from sklearn.model_selection import train_test_split #train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42) However the sampling we have considered here or the one used in Scikit-learn is random sampling by default. This is fine for large dataset however for smaller dataset it is utmost important that the sampled data is representative of the main population data or else we will introduce sampling bias.\nThis is an important bias that could be introduced without prior knowledge and could be overlooked at multiple occassion leading to wrong conclusions. To ensure the sampled dataset is representative of the population set we use stratified sampling (pseudo-random sampling). To make the stratified sampling tractable we first divide the main data into multiple \u0026lsquo;stratas\u0026rsquo; based on an variable which we feel is an feature that should be replicated in our test set. The sample is divided into strata and right number of instances are chosen from each strata. We must not have too many stratas and the each strate must have appropriate number of instances.\nFor the case of property pricing in the district, median_income variable is chosen as the variable whose distribution in the main population and the randomly chosen test sample is same. This attribute is an important attribute to predict the final median housing price. So we can think of converting the continuous variable of median_variable into categorical variable \u0026ndash; that is stratas.\nStratified sampling using median income housing[\u0026#34;median_income\u0026#34;].hist() From the median_income histogram it is seen that most of the entries are clustered in the range of 2-5 (arbitrary units). We can then use this information to make stratas around these instances. Cut routine in the pandas is used for this purpose. This function is also useful for going from a continuous variable to a categorical variable. For example, cut could convert ages to groups of age ranges.\nhousing[\u0026#34;income_cat\u0026#34;] = pd.cut(housing[\u0026#34;median_income\u0026#34;], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], #bins around 2-5 income bracket labels=[1, 2, 3, 4, 5]) housing[\u0026#34;income_cat\u0026#34;].value_counts() 3 7236 2 6581 4 3639 5 2362 1 822 Name: income_cat, dtype: int64  housing[\u0026#34;income_cat\u0026#34;].hist() Now with the population categorised into various median income groups we can use stratified sampling routine (as implemented in scikit-learn) to make our test-set. As an additional proof let\u0026rsquo;s compare this to a randomly sampled test_case. We will redo the random sampling we did prviously but with the new population with categorised median_income.\n#Stratified sampling from scikit-learn  from sklearn.model_selection import StratifiedShuffleSplit, train_test_split split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) for train_index, test_index in split.split(housing, housing[\u0026#34;income_cat\u0026#34;]): strat_train_set = housing.loc[train_index] strat_test_set = housing.loc[test_index] #Using random sampling rand_train_set, rand_test_set = train_test_split(housing, test_size=0.2, random_state=42) Let\u0026rsquo;s check the distribution of the income_cat variable in the strat_test, random_test, and the main sample.\nhousing[\u0026#34;income_cat\u0026#34;].value_counts()/len(housing[\u0026#34;income_cat\u0026#34;]) 3 0.350581 2 0.318847 4 0.176308 5 0.114438 1 0.039826 Name: income_cat, dtype: float64  rand_test_set[\u0026#34;income_cat\u0026#34;].value_counts()/len(rand_test_set[\u0026#34;income_cat\u0026#34;]) 3 0.358527 2 0.324370 4 0.167393 5 0.109496 1 0.040213 Name: income_cat, dtype: float64  strat_test_set[\u0026#34;income_cat\u0026#34;].value_counts()/len(strat_test_set[\u0026#34;income_cat\u0026#34;]) 3 0.350533 2 0.318798 4 0.176357 5 0.114583 1 0.039729 Name: income_cat, dtype: float64  def income_cat_proportions(data): return data[\u0026#34;income_cat\u0026#34;].value_counts() / len(data) compare_props = pd.DataFrame({ \u0026#34;Overall\u0026#34;: income_cat_proportions(housing), \u0026#34;Stratified\u0026#34;: income_cat_proportions(strat_test_set), \u0026#34;Random\u0026#34;: income_cat_proportions(rand_test_set), }).sort_index() compare_props[\u0026#34;Rand. %error\u0026#34;] = 100 * compare_props[\u0026#34;Random\u0026#34;] / compare_props[\u0026#34;Overall\u0026#34;] - 100 compare_props[\u0026#34;Strat. %error\u0026#34;] = 100 * compare_props[\u0026#34;Stratified\u0026#34;] / compare_props[\u0026#34;Overall\u0026#34;] - 100 compare_props Now, we can remove the income_cat column\nfor set_ in (strat_train_set, strat_test_set): set_.drop(\u0026#34;income_cat\u0026#34;, axis=1, inplace=True) Preliminary visualization of the data Let\u0026rsquo;s now dive a bit deeper into the data visualization and analysis. Before we do so, copy the strat_train_set as that would be the data-set we would be playing around and make sure the main data-set is not touched.\nhousing=strat_train_set.copy() housing.info() \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; Int64Index: 16512 entries, 17606 to 15775 Data columns (total 10 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 longitude 16512 non-null float64 1 latitude 16512 non-null float64 2 housing_median_age 16512 non-null float64 3 total_rooms 16512 non-null float64 4 total_bedrooms 16354 non-null float64 5 population 16512 non-null float64 6 households 16512 non-null float64 7 median_income 16512 non-null float64 8 median_house_value 16512 non-null float64 9 ocean_proximity 16512 non-null object dtypes: float64(9), object(1) memory usage: 1.4+ MB  Geographical visualization Let\u0026rsquo;s now plot the data entries in the housing data as per the latitude and longitude.\nhousing.plot(kind=\u0026#39;scatter\u0026#39;,x=\u0026#39;longitude\u0026#39;,y=\u0026#39;latitude\u0026#39;) This look\u0026rsquo;s like California however, we cannot infer anything more out of this. Let\u0026rsquo;s play around a little bit more\u0026hellip;\n Playing with the alpha value in the plotting routine allows us to see the frequency of THAT datapoint in the plot  housing.plot(kind=\u0026#39;scatter\u0026#39;,x=\u0026#39;longitude\u0026#39;,y=\u0026#39;latitude\u0026#39;,alpha=0.1) From here, we can see the high density of listings in the Bay area and LA also around Sacramento and Fresco.\nhousing.plot(kind=\u0026#34;scatter\u0026#34;, x=\u0026#34;longitude\u0026#34;, y=\u0026#34;latitude\u0026#34;, alpha=0.4, s=housing[\u0026#34;population\u0026#34;]/100, label=\u0026#34;population\u0026#34;, figsize=(15,10), c=\u0026#34;median_house_value\u0026#34;, cmap=plt.get_cmap(\u0026#34;jet\u0026#34;), colorbar=True, sharex=False) plt.legend() This is more interesting! We have now plotted the data with more information. Each data-point has two additional set of info apart of frequency of occurence. First being the color of the point is the median_house_value entry (option c). The radius of the data-point is the population of that district (option s). It can be seen that the housing prices are very much related to the location. The ones closer to the bay area are more expensive but need not be densely populated.\nLooking for simple correlations In addition to looking at the plot of housing price, we can check for simpler correaltions. Pearson\u0026rsquo;s correlation matrix is something which is in-built in pandas and can be directly used. It checks for correlation between every pair of feature provided in the data-set. It estimates the covariance of the two features and estimates whether the correlation is inverse, direct, or none.\ncorr_matrix=housing.corr() corr_matrix.style.background_gradient(cmap=\u0026#39;coolwarm\u0026#39;).set_precision(2) corr_matrix[\u0026#39;median_house_value\u0026#39;].sort_values(ascending=True) latitude -0.142724 longitude -0.047432 population -0.026920 total_bedrooms 0.047689 households 0.064506 housing_median_age 0.114110 total_rooms 0.135097 median_income 0.687160 median_house_value 1.000000 Name: median_house_value, dtype: float64  The correlation matrix suggests the amount of correlation between a pair of variables. When close to 1 it means a strong +ve correlation whereas, -1 means an inverse correlation. Looking at the correlation between median_house_values and other variables, we can see that there\u0026rsquo;s some correlation with median_income (0.68 \u0026ndash; so +ve), and with the latitude (-0.14 \u0026ndash; so an inverse relation).\nAnother to check this relation is to plot scatter plots for each pair of variables in the dataset. Below we plot this for few potential/interesting variables\n# from pandas.tools.plotting import scatter_matrix # For older versions of Pandas from pandas.plotting import scatter_matrix attributes = [\u0026#34;median_house_value\u0026#34;, \u0026#34;median_income\u0026#34;, \u0026#34;total_rooms\u0026#34;, \u0026#34;housing_median_age\u0026#34;] scatter_matrix(housing[attributes], figsize=(12, 8)) The diagonal entries show the histogram for each variable. We saw this previously for some variables. The most promising variable from this analysis seems to be the median_income.\nhousing.plot(kind=\u0026#34;scatter\u0026#34;, x=\u0026#34;median_income\u0026#34;, y=\u0026#34;median_house_value\u0026#34;, alpha=0.1) Plotting it shows the stronger correlation with the target variable i.e. median_house_value however we can see horizontal lines (especially at USD 500k, 450k 350k) these could be due to some stratifying done in the dataset implicitly. It would be better to remove those to ensure our model does not spuriously fit for those since they are some of the quirks in the data.\nExperimenting with attributes Before we began proposing models for the data. We can play around with the variables and try different combinations of them to see if we get better trends. Let\u0026rsquo;s look at a few. First, the total_room and/or total_bedroom variable could be changed to average_bedroom_per_house to better for bedrooms rather than looking for total bedroom in that district we would be looking at avg_bedroom per district and similarly we would do it for rooms.\n#Average bedroom per house-holds in the district  housing[\u0026#39;avg_bedroom\u0026#39;]=housing[\u0026#39;total_bedrooms\u0026#39;]/housing[\u0026#39;households\u0026#39;] #Average room per house-holds in the district  housing[\u0026#39;avg_room\u0026#39;]=housing[\u0026#39;total_rooms\u0026#39;]/housing[\u0026#39;households\u0026#39;] #Average bedrooms per rooms in a given district housing[\u0026#39;bedroom_per_room\u0026#39;]=housing[\u0026#39;total_bedrooms\u0026#39;]/housing[\u0026#39;total_rooms\u0026#39;] #Average population per household in a given district housing[\u0026#39;population_per_household\u0026#39;]=housing[\u0026#39;population\u0026#39;]/housing[\u0026#39;households\u0026#39;] #Average room per population in a given district housing[\u0026#39;room_per_popoulation\u0026#39;]=housing[\u0026#39;total_rooms\u0026#39;]/housing[\u0026#39;population\u0026#39;] #Average room per population in a given district housing[\u0026#39;room_per_popoulation\u0026#39;]=housing[\u0026#39;total_rooms\u0026#39;]/housing[\u0026#39;population\u0026#39;] corr_matrix[\u0026#39;median_house_value\u0026#39;].sort_values(ascending=True) bedroom_per_room -0.259984 latitude -0.142724 longitude -0.047432 avg_bedroom -0.043343 population -0.026920 population_per_household -0.021985 total_bedrooms 0.047689 households 0.064506 housing_median_age 0.114110 total_rooms 0.135097 avg_room 0.146285 room_per_popoulation 0.199429 median_income 0.687160 median_house_value 1.000000 Name: median_house_value, dtype: float64  This is interesting! We see that bedroom_per_room is another potential descriptor with negative corelation moreover we get room_per_population and avg_room to be decent new descriptors for the median_house_value. Not bad for a simple math manipulation to better represent the data. This is a crucial step and where domain knowledge and intuition would come handy.\nData cleaning and prepping  Separate the predictors and the target values Write functions to conduct various data transformations ensuring consistency and ease Make sure the data is devoid of any NaN values since that would raise warning and errors. We have three strategies we can implement here: a. Get rid of those points (districts) entirely b. Get rid of whole attribute c. Set missing values to either zero or one of the averages (mean, median, or mode)  In our case, total bedrooms had some missing values.\n# Option a: housing.dropna(subset=[\u0026#34;total_bedrooms\u0026#34;]) # Option b: housing.drop(\u0026#34;total_bedrooms\u0026#34;, axis=1) # Option c: median = housing[\u0026#34;total_bedrooms\u0026#34;].median() housing[\u0026#34;total_bedrooms\u0026#34;].fillna(median, inplace=True) # option 3 Before we do any of this let\u0026rsquo;s first separate the predictor and target_values\nhousing = strat_train_set.drop(\u0026#34;median_house_value\u0026#34;, axis=1) # drop labels for training set housing_labels = strat_train_set[\u0026#34;median_house_value\u0026#34;].copy() #Checking the NULL enties in the dataset sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head() sample_incomplete_rows sample_incomplete_rows.dropna(subset=[\u0026#34;total_bedrooms\u0026#34;]) # option 1 sample_incomplete_rows.drop(\u0026#34;total_bedrooms\u0026#34;, axis=1) #option 2 median = housing[\u0026#34;total_bedrooms\u0026#34;].median() sample_incomplete_rows[\u0026#34;total_bedrooms\u0026#34;].fillna(median, inplace=True) # option 3 sample_incomplete_rows Scikit-learn imputer class This is a handy class to take of missing values. First, we create an instance of that class with specifying what is to be replaced and what strategy is used. Before doing so, we need to make srue the entire data-set has ONLY numerical entries and Imputer will evaluate the given average for all the dataset and store it in the statistics_ instance\nWhat the imputer will do is,\n Evaluate an specified type of average. For a given numerical data-set look for NaN or Null entires in a given attribute and replace it with the computed avearge for that attribute  try: from sklearn.impute import SimpleImputer # Scikit-Learn 0.20+ except ImportError: from sklearn.preprocessing import Imputer as SimpleImputer imputer = SimpleImputer(strategy=\u0026#34;median\u0026#34;) #We define the strategy here  housing_num = housing.drop(\u0026#39;ocean_proximity\u0026#39;, axis=1) # alternatively: housing_num = housing.select_dtypes(include=[np.number]) imputer.fit(housing_num) SimpleImputer(strategy='median')  imputer.statistics_ array([-118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409])  housing_num.median().values array([-118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409])  We can now use this as a training variables set for our model\nX = imputer.transform(housing_num) We convert the Pandas dataframe entries to a numpy array which is transformed with appropriately replacing the missing entries with median.\nprint(type(X), type(housing_num)) \u0026lt;class 'numpy.ndarray'\u0026gt; \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt;  print(np.shape(X), housing_num.shape) \u0026#39;\u0026#39;\u0026#39; If we need the data-frame back housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing.index) \u0026#39;\u0026#39;\u0026#39; (16512, 8) (16512, 8)  Handling Text and Categorical Attribute Now let\u0026rsquo;s preprocess the categorical input feature, ocean_proximity:\nhousing_cat = housing[[\u0026#39;ocean_proximity\u0026#39;]] type(housing_cat) pandas.core.frame.DataFrame  Converting the categorical entries to integers\ntry: from sklearn.preprocessing import OrdinalEncoder except ImportError: from future_encoders import OrdinalEncoder # Scikit-Learn \u0026lt; 0.20 ordinal_encoder = OrdinalEncoder() housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat) housing_cat_encoded[:10] array([[0.], [0.], [4.], [1.], [0.], [1.], [0.], [1.], [0.], [0.]])  housing[\u0026#34;ocean_proximity\u0026#34;].value_counts() \u0026lt;1H OCEAN 7276 INLAND 5263 NEAR OCEAN 2124 NEAR BAY 1847 ISLAND 2 Name: ocean_proximity, dtype: int64  Now, housing_cat_encoded has converted the categorical entries to purely numerical values for each category\nordinal_encoder.categories_ [array(['\u0026lt;1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'], dtype=object)]  Now, this is helpful with the string categories getting converted to numerical categories. However, there is still an small issue. The categorical numbering may introduce some bias in the final model. ML algorithms will assume that two nearby values are more similar than two distant values.\nIn the above case, \u0026lsquo;\u0026lt;1H OCEAN\u0026rsquo; and \u0026lsquo;INLAND\u0026rsquo;have category values as 0 and 1 however \u0026lsquo;\u0026lt;1H OCEAN\u0026rsquo; is more closer to \u0026lsquo;NEAR OCEAN\u0026rsquo; with category value 4.\nTo fix this issue, one solution is to create one binary attribute per category. This is called One-hot encoding as ONLY one of the attribute in the vector is 1 (hot) and others are 0 (cold).\nScikit-learn provides a OneHotEncoder encoder to convert integer categorical values to one-hot vectors.\ntry: from sklearn.preprocessing import OrdinalEncoder # just to raise an ImportError if Scikit-Learn \u0026lt; 0.20 from sklearn.preprocessing import OneHotEncoder except ImportError: from future_encoders import OneHotEncoder # Scikit-Learn \u0026lt; 0.20 cat_encoder = OneHotEncoder() #1-Hot encoded vector for the housing training data-set  housing_cat_1hot = cat_encoder.fit_transform(housing_cat) type(housing_cat_1hot) scipy.sparse.csr.csr_matrix  A sparse array is declared in this case which has the position of the non-zero value and not necessarily the entire numpy matrix. This is helpful in the cases where there are too many categories and also many datapoints. For examples, if we have 4 categories and 1000 datapoints the final one-hot matrix would be 1000x4 size. Most of that would be full of 0s, with only one 1 per row for a particular category.\nThe housing_cat_1hot can be converted to numpy array by using the housing_cat_1hot.toarray()\nAlternatively, you can set sparse=False when creating the OneHotEncoder\ncat_encoder.categories_ [array(['\u0026lt;1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'], dtype=object)]  Let\u0026rsquo;s create a custom transformer to add extra attributes:\nhousing.columns Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'ocean_proximity'], dtype='object')  from sklearn.base import BaseEstimator, TransformerMixin # get the right column indices: safer than hard-coding indices rooms_ix, bedrooms_ix, population_ix, household_ix = [ list(housing.columns).index(col) for col in (\u0026#34;total_rooms\u0026#34;, \u0026#34;total_bedrooms\u0026#34;, \u0026#34;population\u0026#34;, \u0026#34;households\u0026#34;)] #Here we convert the housing.columns to list and  #then find the index for the entry which matches the string in the loop  class CombinedAttributesAdder(BaseEstimator, TransformerMixin): def __init__(self, add_bedrooms_per_room = True): # no *args or **kwargs self.add_bedrooms_per_room = add_bedrooms_per_room def fit(self, X, y=None): return self # nothing else to do def transform(self, X, y=None): rooms_per_household = X[:, rooms_ix] / X[:, household_ix] population_per_household = X[:, population_ix] / X[:, household_ix] if self.add_bedrooms_per_room: bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix] return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room] else: return np.c_[X, rooms_per_household, population_per_household] attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=True) housing_extra_attribs = attr_adder.transform(housing.values) From the above class, there\u0026rsquo;s only one hyperparameter in the class. add_bedrooms_per_room is the only option and is set True by default. Let\u0026rsquo;s check the new feature space by converting it to a Pandas Dataframe\nhousing_extra_attribs = pd.DataFrame( housing_extra_attribs, columns=list(housing.columns)+[\u0026#34;bedrooms_per_room\u0026#34;,\u0026#34;rooms_per_household\u0026#34;, \u0026#34;population_per_household\u0026#34;], index=housing.index) housing_extra_attribs.head() Feature scaling Feaature scaling is an important transformation needed to be applied to the data. With some exceptions, ML algorithms dont perform well when the input numerical entries have very different scales. Eg: One variable has range 0-1 but other variable has range 1-1000. This is the case in our data-base where the total number of rooms range from 6 to 39,320 while the objective variable i.e. median income only ranges from 0-15. Two common ways of scaling:\n Min-max scaling (also called Normalization)  Values are shifted such that they are normalized. They are rescaled in the range of 0-1. We do this by subtracting the min value and dividing by the range in the data\n\\begin{equation} x_{i} = \\frac{X_{i}-min}{max-min} \\end{equation}\nStandardization This is when the mean of the dataset is subtracted from each entry so that the data has mean as 0 and then divided by the standard deviation so that the resulting distribution has a unit variance. Unlike min-max scaling, standardization does not bound to a particular range like 0-1. However, standardisation is much less affected by outliers. If a particular values is extremely high or low that could affect the other inputs in the case of min-max scaling. However that effect is reduced in the case of standardization given it does not directly account for the range in the scaling but the mean and variance.  \\begin{equation} x_{i} = \\frac{X_{i}-\\mu}{\\sigma} \\end{equation}\n NOTE: It is important that these scaling operations are performed on the training data only and not on the full dataset\n Transformation pipelines Pipeline class in scikit-learn can help with sequences of transformations. Now let\u0026rsquo;s build a pipeline for preprocessing the numerical attributes (note that we could use CombinedAttributesAdder() instead of FunctionTransformer(...) if we preferred):\nfrom sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler num_pipeline = Pipeline([ (\u0026#39;imputer\u0026#39;, SimpleImputer(strategy=\u0026#34;median\u0026#34;)), #Fill in missing values using the median of each entry  (\u0026#39;attribs_adder\u0026#39;, CombinedAttributesAdder(add_bedrooms_per_room=True)), #Add additional columns entrys (\u0026#39;std_scaler\u0026#39;, StandardScaler()), #Feature scaling -- using standardisation here  ]) housing_num_tr = num_pipeline.fit_transform(housing_num) housing_num_tr array([[-1.15604281, 0.77194962, 0.74333089, ..., -0.31205452, -0.08649871, 0.15531753], [-1.17602483, 0.6596948 , -1.1653172 , ..., 0.21768338, -0.03353391, -0.83628902], [ 1.18684903, -1.34218285, 0.18664186, ..., -0.46531516, -0.09240499, 0.4222004 ], ..., [ 1.58648943, -0.72478134, -1.56295222, ..., 0.3469342 , -0.03055414, -0.52177644], [ 0.78221312, -0.85106801, 0.18664186, ..., 0.02499488, 0.06150916, -0.30340741], [-1.43579109, 0.99645926, 1.85670895, ..., -0.22852947, -0.09586294, 0.10180567]])  This Pipeline constructor takes a list of name/estimator pairs defining a sequence of steps. All but the last step/estimator must be the trasnformers (like feature scaling).\ntry: from sklearn.compose import ColumnTransformer except ImportError: from future_encoders import ColumnTransformer # Scikit-Learn \u0026lt; 0.20 Now let\u0026rsquo;s join all these components into a big pipeline that will preprocess both the numerical and the categorical features (again, we could use CombinedAttributesAdder() instead of FunctionTransformer(...) if we preferred):\nnum_attribs = list(housing_num) cat_attribs = [\u0026#34;ocean_proximity\u0026#34;] full_pipeline = ColumnTransformer([ (\u0026#34;num\u0026#34;, num_pipeline, num_attribs), (\u0026#34;cat\u0026#34;, OneHotEncoder(), cat_attribs), ]) housing_prepared = full_pipeline.fit_transform(housing) housing_prepared array([[-1.15604281, 0.77194962, 0.74333089, ..., 0. , 0. , 0. ], [-1.17602483, 0.6596948 , -1.1653172 , ..., 0. , 0. , 0. ], [ 1.18684903, -1.34218285, 0.18664186, ..., 0. , 0. , 1. ], ..., [ 1.58648943, -0.72478134, -1.56295222, ..., 0. , 0. , 0. ], [ 0.78221312, -0.85106801, 0.18664186, ..., 0. , 0. , 0. ], [-1.43579109, 0.99645926, 1.85670895, ..., 0. , 1. , 0. ]])  housing_prepared.shape (16512, 16)  Training and evaluation on the training set Given the prioir transformations, the features are scaled, categories are converted to one-hot vectors, and the missing variables are taken account of. Let\u0026rsquo;s train a Linear Regression model first\nfrom sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(housing_prepared, housing_labels) LinearRegression()  #Test some data out on the model  trial_input = housing.iloc[:5] #First 5 entries  trial_label = housing_labels.iloc[:5] #First 5 labels corresponding to the entries  prep_trial_input = full_pipeline.transform(trial_input) #Transforming the entries to suit the trained input  print(\u0026#39;Predictions:\u0026#39;,lin_reg.predict(prep_trial_input)) Predictions: [210644.60459286 317768.80697211 210956.43331178 59218.98886849 189747.55849879]  print(\u0026#39;Labels:\u0026#39;,list(trial_label)) Labels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]  from sklearn.metrics import mean_squared_error housing_predictions = lin_reg.predict(housing_prepared) lin_mse = mean_squared_error(housing_labels, housing_predictions) lin_rmse = np.sqrt(lin_mse) lin_rmse 68628.19819848923  housing_labels.describe() count 16512.000000 mean 206990.920724 std 115703.014830 min 14999.000000 25% 119800.000000 50% 179500.000000 75% 263900.000000 max 500001.000000 Name: median_house_value, dtype: float64  As seen the RMSE is 68628 which is better than nothing but still it is quite high when the range of the median_house_values range from 15000 to 500000\nfrom sklearn.metrics import mean_absolute_error lin_mae = mean_absolute_error(housing_labels, housing_predictions) lin_mae 49439.89599001897  Here the model is underfitting the data since the RMSE is so high.\n When this happens either the features do not provide enough information to make good predictions or the model is not powerful enough.\n Let\u0026rsquo;s try using a more complex model, DecisionTreeRegressor which is capable of finding non-linear relationships in the data\nDecision Tree Regressor from sklearn.tree import DecisionTreeRegressor tree_reg = DecisionTreeRegressor(random_state=42) tree_reg.fit(housing_prepared, housing_labels) DecisionTreeRegressor(random_state=42)  housing_predictions = tree_reg.predict(housing_prepared) tree_mse = mean_squared_error(housing_labels, housing_predictions) tree_rmse = np.sqrt(tree_mse) tree_rmse 0.0  This model is badly overfitting the data! Let\u0026rsquo;s proof this hypothesis using cross-validation schemes. We dont want to touch the test set, JUST WORK WITH THE TRAINING SET. Only touch the test set when the model we are using is good enough. We will use the part of the training set for training and other part for model validation.\nFine-tune the model Cross-validation\nCross-validation is a method of getting reliable estimate of model performance using only the training data 10-fold cross-validation \u0026mdash; breaking training data in 10 equal parts creating 10 miniature test/train splits. Out of the 10 folds, train data on 9 and test on 10th. Do this 10 times each time holding out different fold.\n Scikit-learn cross-validation feature expects a utility function (greater the better) rather than a cost function (lower the better), so to score the functions we use opposite of MSE, which is why we again compute -scores before calculating the square root\n from sklearn.model_selection import cross_val_score scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\u0026#34;neg_mean_squared_error\u0026#34;, cv=10) tree_rmse_scores = np.sqrt(-scores) def display_scores(scores): print(\u0026#34;Scores:\u0026#34;, scores) print(\u0026#34;Mean:\u0026#34;, scores.mean()) print(\u0026#34;Standard deviation:\u0026#34;, scores.std()) display_scores(tree_rmse_scores) Scores: [70194.33680785 66855.16363941 72432.58244769 70758.73896782 71115.88230639 75585.14172901 70262.86139133 70273.6325285 75366.87952553 71231.65726027] Mean: 71407.68766037929 Standard deviation: 2439.4345041191004  Cross-validation not only allows us to get an estimate of the performance of the model but also the measure of how precise this estimate is (i.e. standard deviation). The Decision tree has high std-dev. This information could not be obtained with just one validation set. However, caveat is that cross-validation comes at the cost of training the model several times, so it is not always possible.\nLet\u0026rsquo;s compute the same score for LinearRegresson model.\nlin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\u0026#34;neg_mean_squared_error\u0026#34;, cv=10) lin_rmse_scores = np.sqrt(-lin_scores) display_scores(lin_rmse_scores) Scores: [66782.73843989 66960.118071 70347.95244419 74739.57052552 68031.13388938 71193.84183426 64969.63056405 68281.61137997 71552.91566558 67665.10082067] Mean: 69052.46136345083 Standard deviation: 2731.674001798349  Here it can be seen that DecisionTree model performs much worse than the LinearRegression model.\nRandom Forrest Regressor Random forrest works by employing many decision trees on random subsets of the features, then averaging out their predictions. Building a model on top of many other models is called Ensemble Learning and it is often great way to push ML algorithms even further.\nfrom sklearn.ensemble import RandomForestRegressor forest_reg = RandomForestRegressor(n_estimators=10, random_state=42) forest_reg.fit(housing_prepared, housing_labels) RandomForestRegressor(n_estimators=10, random_state=42)   Note:we specify n_estimators=10 to avoid a warning about the fact that the default value is going to change to 100 in Scikit-Learn 0.22.\n housing_predictions = forest_reg.predict(housing_prepared) forest_mse = mean_squared_error(housing_labels, housing_predictions) forest_rmse = np.sqrt(forest_mse) forest_rmse 21933.31414779769  from sklearn.model_selection import cross_val_score forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring=\u0026#34;neg_mean_squared_error\u0026#34;, cv=10) forest_rmse_scores = np.sqrt(-forest_scores) display_scores(forest_rmse_scores) Scores: [51646.44545909 48940.60114882 53050.86323649 54408.98730149 50922.14870785 56482.50703987 51864.52025526 49760.85037653 55434.21627933 53326.10093303] Mean: 52583.72407377466 Standard deviation: 2298.353351147122  Random forest regressor looks better than DecisionTree and LinearRegression. The RMSE is still quite high for production quality code and could be due to overfitting. We can try other algorithms before spending time on a particular algorithm tweaking the hyperparameters. The goal is to shortlist 2-3 methods that are promising then fine-tune the model. Before we move ahead we can take a look at one more ML algorithm which is commonly employed for such supervised learning cases: Support Vector Regression\nfrom sklearn.svm import SVR svm_reg = SVR(kernel=\u0026#34;linear\u0026#34;) svm_reg.fit(housing_prepared, housing_labels) housing_predictions = svm_reg.predict(housing_prepared) svm_mse = mean_squared_error(housing_labels, housing_predictions) svm_rmse = np.sqrt(svm_mse) svm_rmse 111094.6308539982  Step 4: Fine-tune the model Once we settle for an algorithm we can fine-tune them efficiently using some of the in-built scikit-learn routines.\n** Grid Search **\nGridSearchCV is a faster way of tweaking the hyper-parameters for a given algorithm. It needs the hyper-parameters you want to experiment with, what values to try out, and it will evaluate possible combination of hyperparameters values using cross-validation. We can do that step for RandomForrestRegressor which we found to have lowesst RMSE of the three methods we tried\nfrom sklearn.model_selection import GridSearchCV param_grid = [ # try 12 (3×4) combinations of hyperparameters {\u0026#39;n_estimators\u0026#39;: [3, 10, 30], \u0026#39;max_features\u0026#39;: [2, 4, 6, 8]}, # then try 6 (2×3) combinations with bootstrap set as False {\u0026#39;bootstrap\u0026#39;: [False], \u0026#39;n_estimators\u0026#39;: [3, 10], \u0026#39;max_features\u0026#39;: [2, 3, 4]}, ] forest_reg = RandomForestRegressor(random_state=42) # train across 5 folds, that\u0026#39;s a total of (12+6)*5=90 rounds of training  grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring=\u0026#39;neg_mean_squared_error\u0026#39;, return_train_score=True) grid_search.fit(housing_prepared, housing_labels) GridSearchCV(cv=5, estimator=RandomForestRegressor(random_state=42), param_grid=[{'max_features': [2, 4, 6, 8], 'n_estimators': [3, 10, 30]}, {'bootstrap': [False], 'max_features': [2, 3, 4], 'n_estimators': [3, 10]}], return_train_score=True, scoring='neg_mean_squared_error')  The param_grid tells Scikit-learn to:\n First evaluate 3x4 combinations of n_estimators and max_features with bootstrap True which is the default. Then with bootstrap set as False we look for 2x3 combinations of n_estimators and max_featurs for the random forest Finally, both these models are trained 5 times for the cross validation purposes in a 5-fold cross-validation fashion.  Total of (12+6)x5=90 round of training are conducted. Finally when it is done we get the best model parameters which give lowest RMSE.\ngrid_search.best_params_ {'max_features': 8, 'n_estimators': 30}  grid_search.best_estimator_ RandomForestRegressor(max_features=8, n_estimators=30, random_state=42)  cvres = grid_search.cv_results_ for mean_score, params in zip(cvres[\u0026#34;mean_test_score\u0026#34;], cvres[\u0026#34;params\u0026#34;]): print(np.sqrt(-mean_score), params) 63669.11631261028 {'max_features': 2, 'n_estimators': 3} 55627.099719926795 {'max_features': 2, 'n_estimators': 10} 53384.57275149205 {'max_features': 2, 'n_estimators': 30} 60965.950449450494 {'max_features': 4, 'n_estimators': 3} 52741.04704299915 {'max_features': 4, 'n_estimators': 10} 50377.40461678399 {'max_features': 4, 'n_estimators': 30} 58663.93866579625 {'max_features': 6, 'n_estimators': 3} 52006.19873526564 {'max_features': 6, 'n_estimators': 10} 50146.51167415009 {'max_features': 6, 'n_estimators': 30} 57869.25276169646 {'max_features': 8, 'n_estimators': 3} 51711.127883959234 {'max_features': 8, 'n_estimators': 10} 49682.273345071546 {'max_features': 8, 'n_estimators': 30} 62895.06951262424 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3} 54658.176157539405 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10} 59470.40652318466 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3} 52724.9822587892 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10} 57490.5691951261 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3} 51009.495668875716 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}  Randomised search Grid search approach is fine when we are exploring relatively few combinations. But when hyperparameter space is large it is often preferrable to use RandomizedSearchCV instead. Here instead of doing all the possible combinationes of hyperparameters, it evaluates a given number of random combinations by selecting a random value for each hyper parameter at every iteration.\nEnsemble search Combine models that perform best. The group or \u0026lsquo;ensemble\u0026rsquo; will often perform better than the best individual model just like RandomForest peforms better than Decision Trees especially if we have individual models make different types of errors.\nStep 5: Analyze the Best Models and their Errors RandomForestRegressor can indicate the relative importance of each attribute for making the accurate predictions.\nfeature_importances = grid_search.best_estimator_.feature_importances_ feature_importances array([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02, 1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01, 5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02, 1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03])  extra_attribs = [\u0026#34;rooms_per_hhold\u0026#34;, \u0026#34;pop_per_hhold\u0026#34;, \u0026#34;bedrooms_per_room\u0026#34;] cat_encoder = full_pipeline.named_transformers_[\u0026#34;cat\u0026#34;] cat_one_hot_attribs = list(cat_encoder.categories_[0]) attributes = num_attribs + extra_attribs + cat_one_hot_attribs sorted(zip(feature_importances, attributes), reverse=True) [(0.36615898061813423, 'median_income'), (0.16478099356159054, 'INLAND'), (0.10879295677551575, 'pop_per_hhold'), (0.07334423551601243, 'longitude'), (0.06290907048262032, 'latitude'), (0.056419179181954014, 'rooms_per_hhold'), (0.053351077347675815, 'bedrooms_per_room'), (0.04114379847872964, 'housing_median_age'), (0.014874280890402769, 'population'), (0.014672685420543239, 'total_rooms'), (0.014257599323407808, 'households'), (0.014106483453584104, 'total_bedrooms'), (0.010311488326303788, '\u0026lt;1H OCEAN'), (0.0028564746373201584, 'NEAR OCEAN'), (0.0019604155994780706, 'NEAR BAY'), (6.0280386727366e-05, 'ISLAND')]  Step 6: Evaluate the model on the Test Set final_model = grid_search.best_estimator_ X_test = strat_test_set.drop(\u0026#34;median_house_value\u0026#34;, axis=1) y_test = strat_test_set[\u0026#34;median_house_value\u0026#34;].copy() X_test_prepared = full_pipeline.transform(X_test) final_predictions = final_model.predict(X_test_prepared) fig, ax = plt.subplots(1,1, figsize=(8,8)) ax.scatter(y_test, final_predictions, s=100) lims = [np.min([ax.get_xlim(), ax.get_ylim()]), # min of both axes np.max([ax.get_xlim(), ax.get_ylim()]), # max of both axes ] ax.plot(lims, lims, \u0026#39;k--\u0026#39;, linewidth=2.0, alpha=0.75, zorder=0) ax.set_aspect(\u0026#39;equal\u0026#39;) ax.set_xlim(lims) ax.set_ylim(lims) ax.set_xlabel(\u0026#39;ML Prediction\u0026#39;) ax.set_ylabel(\u0026#39;Actual Value\u0026#39;) final_mse = mean_squared_error(y_test, final_predictions) final_rmse = np.sqrt(final_mse) print(final_rmse) 47730.22690385927  We can compute a 95% confidence interval for the test RMSE:\nfrom scipy import stats confidence = 0.95 squared_errors = (final_predictions - y_test) ** 2 mean = squared_errors.mean() m = len(squared_errors) np.sqrt(stats.t.interval(confidence, m - 1, loc=np.mean(squared_errors), scale=stats.sem(squared_errors))) array([45685.10470776, 49691.25001878])  Alternatively, we could use a z-scores rather than t-scores:\nzscore = stats.norm.ppf((1 + confidence) / 2) zmargin = zscore * squared_errors.std(ddof=1) / np.sqrt(m) np.sqrt(mean - zmargin), np.sqrt(mean + zmargin) `` (45685.717918136594, 49690.68623889426)","permalink":"/blog/ml_project_temp/","tags":["Python","Scikit-learn","Machine learning"],"title":"Property Price Prediction ML-model"},{"categories":null,"contents":"Convolutional neural network is used to train on the CIFAR-10 dataset using PyTorch.\nWhat does it consists of? The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\nThe dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\nWhat\u0026rsquo;s a Convolutional Neural Network (CNN)? Same as a neural network but are optimized for image analysis. Before training the weights and biases in the full-connected layer the training data is \u0026lsquo;screened and filtered\u0026rsquo; to tease out relevant features of each image by passing each image through a prescribed filter and \u0026lsquo;convolutions\u0026rsquo;. Think of it like passing a colored lens or fancy ink to selectively look at edges, contrasts, shapes in the image.\nFinally that a projection of that image is made by \u0026lsquo;pooling\u0026rsquo; which is a way of down-sampling the resulting convolution as a new data-point.\nGood reading links:\n http://cs231n.github.io/convolutional-networks/ https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/  ","permalink":"/blog/pytorch_cnn_example/","tags":["Python","PyTorch","Machine learning"],"title":"Implement CNN in PyTorch"},{"categories":null,"contents":"This tutorial is adapted from Jake VanderPlas\u0026rsquo;s example of SVM as given in his reference book: Python Data Science Handbook\nMotivation for Support Vector Machines We want to find a line/curve (in 2D) or a manifold (in n-D) that divides the class from each other. This is a type of Discriminative Classification\nConsider a simple case of classification task, in which the two classes of points are well separated. We can find region in space which best separates the data into two classes. The Support Vectors in Support Vector Machine are the (hyper)planes which lie at the edge of the individual classes. This idea is much easier to understand from 2D perspective.\n","permalink":"/blog/svm_usecase/","tags":["Python","Machine learning","Learning"],"title":"About Support Vector Machines"},{"categories":null,"contents":"High throughput screening of MaterialsProject dataset using pymatgen API for estimating perovskite bulk stability. For selected candidates, analyze their surface morphology using ab-initio thermodynamic analysis.\n","permalink":"/blog/perovskites/","tags":["Python","Thermodynamics","DFT"],"title":"Perovskite Stability Analysis"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml\n[outputs] home = [\u0026#34;HTML\u0026#34;, \u0026#34;JSON\u0026#34;] Searching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category\n... \u0026#34;contents\u0026#34;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026#34;tags\u0026#34;:{{ .Params.tags | jsonify }}{{end}}, \u0026#34;categories\u0026#34; : {{ .Params.categories | jsonify }}, ... Edit fuse.js options to Search static/js/search.js\nkeys: [ \u0026#34;title\u0026#34;, \u0026#34;contents\u0026#34;, \u0026#34;tags\u0026#34;, \u0026#34;categories\u0026#34; ] ","permalink":"/search/","tags":null,"title":"Search Results"}]